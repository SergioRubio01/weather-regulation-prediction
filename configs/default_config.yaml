# Default configuration for weather regulation prediction
name: default_experiment
description: Default configuration for weather regulation prediction system
version: 1.0.0

# Data configuration
data:
  airports: [EGLL, LSZH, LFPG, LOWW]
  time_init: "2017-01-01 00:50:00"
  time_end: "2019-12-08 23:50:00"
  time_delta: 30  # minutes
  test_size: 0.2
  validation_size: 0.2
  random_state: 42
  download_type: 1
  data_path: "./Data"
  output_path: "./Output"
  use_minmax_scaler: true
  use_label_binarizer: true
  window_size: 1
  use_oversampling: false
  use_undersampling: false
  smote_ratio: 1.0

# Training configuration
training:
  use_cross_validation: true
  cv_folds: 5
  stratified: true
  use_early_stopping: true
  early_stopping_patience: 10
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  use_lr_scheduler: true
  lr_scheduler_type: reduce_on_plateau
  lr_scheduler_patience: 5
  lr_scheduler_factor: 0.5
  save_best_model: true
  checkpoint_monitor: val_accuracy
  checkpoint_mode: max
  tensorboard: true
  wandb: false
  mlflow: false
  log_interval: 10
  use_gpu: true
  mixed_precision: false
  num_workers: 4

# Model configurations
models:
  random_forest:
    n_estimators: [50, 100, 200]
    criterion: [gini, entropy, log_loss]
    max_depth: [5, 7, 10, 15]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [auto, sqrt, 0.5]
    bootstrap: true
    n_jobs: -1
    verbose: 0

  lstm:
    units: [50, 100, 200]
    epochs: [30, 60, 120]
    batch_size: [32, 64, 128]
    dropout_rate: [0.2, 0.3, 0.5]
    recurrent_dropout: [0.0, 0.2]
    activation: tanh
    recurrent_activation: sigmoid
    use_bias: true
    return_sequences: false
    stateful: false
    optimizer: adam
    loss: binary_crossentropy
    learning_rate: [0.001, 0.01, 0.1]
    num_layers: [1, 2, 3]
    bidirectional: false
    early_stopping_patience: 10
    reduce_lr_patience: 5
    reduce_lr_factor: 0.5

  cnn:
    filters: [32, 64, 128]
    kernel_size: [3, 5, 7]
    pool_size: [2, 3]
    dropout_rate: [0.2, 0.3, 0.5]
    activation: relu
    padding: same
    epochs: [30, 60, 120]
    batch_size: [32, 64, 128]
    optimizer: adam
    loss: binary_crossentropy
    learning_rate: [0.001, 0.01]
    num_conv_layers: [1, 2, 3]
    use_batch_norm: true
    dense_units: [64, 128, 256]

  gru:
    units: [50, 100, 200]
    epochs: [30, 60, 120]
    batch_size: [32, 64, 128]
    dropout_rate: [0.2, 0.3, 0.5]
    recurrent_dropout: [0.0, 0.2]
    activation: tanh
    recurrent_activation: sigmoid
    optimizer: adam
    loss: binary_crossentropy
    learning_rate: [0.001, 0.01]
    num_layers: [1, 2, 3]
    bidirectional: false

  transformer:
    d_model: [64, 128, 256]
    num_heads: [4, 8]
    num_layers: [2, 4, 6]
    d_ff: [128, 256, 512]
    dropout_rate: [0.1, 0.2, 0.3]
    attention_dropout: [0.1, 0.2]
    epochs: [50, 100, 200]
    batch_size: [16, 32, 64]
    optimizer: adam
    loss: binary_crossentropy
    learning_rate: [0.0001, 0.001]
    warmup_steps: 4000
    use_positional_encoding: true

  ensemble:
    base_models: [random_forest, lstm, cnn]
    voting_type: soft
    weights: null
    use_stacking: false
    meta_learner: logistic_regression
    cv_folds: 5

  autoencoder:
    encoding_dim: [8, 16, 32]
    hidden_layers: [[64, 32], [128, 64, 32]]
    activation: relu
    output_activation: sigmoid
    epochs: [50, 100, 200]
    batch_size: [32, 64]
    optimizer: adam
    loss: mse
    learning_rate: [0.001, 0.01]
    use_for_feature_extraction: true
    fine_tune_classifier: true

# Experiment settings
experiment_settings:
  models_to_train: [random_forest, lstm]
  hyperparameter_tuning: true
  tuning_method: grid  # grid, random, bayesian
  tuning_trials: 100

# Output settings
output_settings:
  save_predictions: true
  save_feature_importance: true
  save_confusion_matrix: true
  save_roc_curve: true
  save_training_history: true
