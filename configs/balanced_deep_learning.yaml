# Deep Learning Models Configuration for Balanced Dataset
name: balanced_deep_learning_experiment
description: Train LSTM, GRU, CNN and Transformer models on balanced weather regulation dataset
version: 1.0.0

# Data configuration - using pre-created balanced dataset
data:
  # Use the existing balanced dataset
  use_balanced_dataset: true
  balanced_dataset_path: "./data/balanced_weather_data.csv"
  
  # No need to specify airports - already in balanced dataset
  airports: []  # Will be loaded from balanced dataset
  
  # Time configuration (for compatibility)
  time_init: "2017-01-01 00:00:00"
  time_end: "2019-12-31 23:59:00"
  time_delta: 30
  
  # Data splits
  test_size: 0.15
  validation_size: 0.15
  random_state: 42
  
  # Paths
  data_path: "./data"
  output_path: "./output/balanced_deep_learning"
  
  # Preprocessing
  use_minmax_scaler: false  # Use StandardScaler for deep learning
  use_standard_scaler: true
  
  # Sequence configuration for RNNs
  sequence_length: 24  # 12 hours of data (24 * 30 minutes)
  
  # Feature configuration - already engineered in balanced dataset
  use_existing_features: true

# Training configuration
training:
  # Cross-validation
  use_cross_validation: false  # Use fixed validation set for deep learning
  
  # Early stopping
  use_early_stopping: true
  early_stopping_patience: 10
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  
  # Learning rate scheduling
  use_lr_scheduler: true
  lr_scheduler_type: reduce_on_plateau
  lr_scheduler_patience: 5
  lr_scheduler_factor: 0.5
  
  # Checkpointing
  save_best_model: true
  checkpoint_monitor: val_f1_score
  checkpoint_mode: max
  
  # Hardware
  use_gpu: true
  mixed_precision: true
  num_workers: 4

# Model configurations
models:
  lstm:
    # Architecture
    units: [64]
    num_layers: [2]
    bidirectional: true
    
    # Regularization
    dropout_rate: [0.3]
    recurrent_dropout: [0.2]
    
    # Training
    epochs: [50]
    batch_size: [32]
    learning_rate: [0.001]
    
    # Optimization
    optimizer: adam
    loss: binary_crossentropy
    metrics: [accuracy, AUC]
    
    # Other
    activation: tanh
    recurrent_activation: sigmoid
    return_sequences: false
    
  gru:
    # Architecture
    units: [64]
    num_layers: [2]
    bidirectional: true
    
    # Regularization
    dropout_rate: [0.3]
    recurrent_dropout: [0.2]
    
    # Training
    epochs: [50]
    batch_size: [32]
    learning_rate: [0.001]
    
    # Optimization
    optimizer: adam
    loss: binary_crossentropy
    
  cnn:
    # Architecture
    filters: [[32, 64, 128]]
    kernel_size: [3]
    pool_size: [2]
    
    # Regularization
    dropout_rate: [0.3]
    
    # Dense layers
    dense_units: [64]
    
    # Training
    epochs: [50]
    batch_size: [32]
    learning_rate: [0.001]
    
    # Other
    activation: relu
    padding: same
    use_batch_norm: true
    
  transformer:
    # Architecture
    d_model: [64]
    num_heads: [4]
    num_layers: [2]
    d_ff: [128]
    
    # Regularization
    dropout_rate: [0.2]
    attention_dropout: [0.1]
    
    # Training
    epochs: [50]
    batch_size: [32]
    learning_rate: [0.001]
    warmup_steps: 1000
    
    # Other
    use_positional_encoding: true
    activation: gelu

# Experiment settings
experiment_settings:
  models_to_train: [lstm, gru, cnn, transformer]
  
  # Hyperparameter tuning
  hyperparameter_tuning: true
  tuning_method: bayesian
  tuning_trials: 20  # Reduced for faster results
  
  # Threshold optimization (critical for imbalanced data)
  optimize_threshold: true
  threshold_range: [0.1, 0.9]
  threshold_metric: f1_score
  
  # Ensemble
  create_ensemble: true
  ensemble_base_models: [lstm, gru, cnn]
  ensemble_method: stacking
  ensemble_meta_learner: gradient_boosting

# Output settings
output_settings:
  save_predictions: true
  save_feature_importance: false  # Not applicable for most DL models
  save_confusion_matrix: true
  save_roc_curve: true
  save_training_history: true
  save_model_architecture: true
  
  # Visualization
  plot_training_curves: true
  plot_model_comparison: true
  plot_threshold_optimization: true

# Expected performance (based on balanced dataset paper)
expected_performance:
  # Traditional ML baselines from balanced dataset
  gradient_boosting_baseline:
    f1_score: 0.879
    auc_roc: 0.954
  
  # Target for deep learning models
  target_metrics:
    f1_score: ">0.85"  # Should be competitive with gradient boosting
    auc_roc: ">0.95"
    recall: ">0.90"    # High recall is important for safety