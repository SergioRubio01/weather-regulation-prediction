{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Regulation Prediction System - Quick Start Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to the Weather Regulation Prediction System. You'll learn how to:\n",
    "\n",
    "1. Load and configure the system\n",
    "2. Prepare weather and regulation data\n",
    "3. Train different machine learning models\n",
    "4. Evaluate and compare model performance\n",
    "5. Generate reports and visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have installed all required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Configuration\n",
    "\n",
    "The system uses a configuration-based approach for managing experiments. Let's start by creating a basic configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ExperimentConfig, DataConfig, TrainingConfig, RandomForestConfig, LSTMConfig\n",
    "from config_parser import ConfigParser\n",
    "\n",
    "# Create a basic configuration\n",
    "config = ExperimentConfig(\n",
    "    name=\"quick_start_experiment\",\n",
    "    description=\"Introduction to the weather regulation prediction system\",\n",
    "    data=DataConfig(\n",
    "        airports=[\"EGLL\"],  # London Heathrow\n",
    "        start_date=\"2023-01-01\",\n",
    "        end_date=\"2023-01-31\",\n",
    "        time_resolution=\"30min\"\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        test_size=0.2,\n",
    "        validation_size=0.2,\n",
    "        random_state=42,\n",
    "        cross_validation=True,\n",
    "        cv_folds=5\n",
    "    ),\n",
    "    models={\n",
    "        'random_forest': RandomForestConfig(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Configuration created for experiment: {config.name}\")\n",
    "print(f\"Target airport: {config.data.airports[0]}\")\n",
    "print(f\"Models to train: {list(config.models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "\n",
    "For this tutorial, we'll generate synthetic weather and regulation data that resembles real aviation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic weather data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create realistic weather patterns\n",
    "timestamps = pd.date_range('2023-01-01', periods=n_samples, freq='30min')\n",
    "\n",
    "# Temperature with daily cycle\n",
    "hour_of_day = timestamps.hour\n",
    "temp_base = 10 + 8 * np.sin(2 * np.pi * hour_of_day / 24)  # Daily temperature cycle\n",
    "temperature = temp_base + np.random.normal(0, 3, n_samples)  # Add noise\n",
    "\n",
    "# Pressure (realistic values)\n",
    "pressure = np.random.normal(1013, 15, n_samples)\n",
    "\n",
    "# Wind speed (typically lower at night)\n",
    "wind_base = 5 + 3 * np.sin(2 * np.pi * hour_of_day / 24 + np.pi/2)\n",
    "wind_speed = np.abs(wind_base + np.random.normal(0, 2, n_samples))\n",
    "\n",
    "# Wind direction\n",
    "wind_direction = np.random.uniform(0, 360, n_samples)\n",
    "\n",
    "# Visibility (lower during poor weather)\n",
    "visibility = np.random.lognormal(9.2, 0.5, n_samples)  # Log-normal distribution\n",
    "visibility = np.clip(visibility, 1000, 20000)  # Realistic range\n",
    "\n",
    "# Humidity\n",
    "humidity = np.random.beta(3, 2, n_samples) * 100  # Beta distribution\n",
    "\n",
    "# Weather codes (simplified)\n",
    "weather_codes = np.random.choice(['CLR', 'FEW', 'SCT', 'BKN', 'OVC', 'RA', 'SN', 'FG'], \n",
    "                                n_samples, p=[0.3, 0.2, 0.15, 0.15, 0.1, 0.05, 0.02, 0.03])\n",
    "\n",
    "# Create weather DataFrame\n",
    "weather_data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'airport': 'EGLL',\n",
    "    'temperature': temperature,\n",
    "    'pressure': pressure,\n",
    "    'wind_speed': wind_speed,\n",
    "    'wind_direction': wind_direction,\n",
    "    'visibility': visibility,\n",
    "    'humidity': humidity,\n",
    "    'weather_code': weather_codes\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(weather_data)} weather observations\")\n",
    "print(\"\\nWeather data sample:\")\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regulation data\n",
    "# Regulations are more likely during poor weather conditions\n",
    "\n",
    "# Create weather severity score\n",
    "weather_severity = (\n",
    "    (weather_data['visibility'] < 5000).astype(int) * 0.3 +  # Low visibility\n",
    "    (weather_data['wind_speed'] > 15).astype(int) * 0.2 +    # High wind\n",
    "    weather_data['weather_code'].isin(['RA', 'SN', 'FG']).astype(int) * 0.3 +  # Precipitation/fog\n",
    "    (weather_data['temperature'] < 0).astype(int) * 0.2      # Freezing conditions\n",
    ")\n",
    "\n",
    "# Probability of regulation based on weather severity\n",
    "regulation_prob = 0.1 + 0.4 * weather_severity  # Base 10% + up to 40% for severe weather\n",
    "\n",
    "# Generate regulations with some randomness\n",
    "has_regulation = np.random.binomial(1, regulation_prob)\n",
    "\n",
    "# Create regulation DataFrame\n",
    "regulation_data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'airport': 'EGLL',\n",
    "    'has_regulation': has_regulation,\n",
    "    'regulation_type': np.where(has_regulation, \n",
    "                               np.random.choice(['WX', 'ATC', 'EQ'], len(has_regulation)), \n",
    "                               'None'),\n",
    "    'severity_score': weather_severity\n",
    "})\n",
    "\n",
    "regulation_rate = has_regulation.mean()\n",
    "print(f\"Generated {len(regulation_data)} regulation records\")\n",
    "print(f\"Regulation rate: {regulation_rate:.1%}\")\n",
    "print(f\"Total regulations: {has_regulation.sum()}\")\n",
    "\n",
    "print(\"\\nRegulation data sample:\")\n",
    "print(regulation_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Visualization\n",
    "\n",
    "Let's explore our generated data to understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Weather Data Exploration', fontsize=16)\n",
    "\n",
    "# Temperature over time\n",
    "axes[0, 0].plot(weather_data['timestamp'], weather_data['temperature'])\n",
    "axes[0, 0].set_title('Temperature over Time')\n",
    "axes[0, 0].set_ylabel('Temperature (Â°C)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pressure distribution\n",
    "axes[0, 1].hist(weather_data['pressure'], bins=30, alpha=0.7)\n",
    "axes[0, 1].set_title('Pressure Distribution')\n",
    "axes[0, 1].set_xlabel('Pressure (hPa)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Wind speed vs regulations\n",
    "reg_data = weather_data.merge(regulation_data[['timestamp', 'has_regulation']], on='timestamp')\n",
    "for reg_status, label in [(0, 'No Regulation'), (1, 'Regulation')]:\n",
    "    data_subset = reg_data[reg_data['has_regulation'] == reg_status]\n",
    "    axes[0, 2].scatter(data_subset['wind_speed'], data_subset['visibility'], \n",
    "                      alpha=0.6, label=label, s=20)\n",
    "axes[0, 2].set_title('Wind Speed vs Visibility (by Regulation Status)')\n",
    "axes[0, 2].set_xlabel('Wind Speed (kt)')\n",
    "axes[0, 2].set_ylabel('Visibility (m)')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Regulations by hour of day\n",
    "hourly_regs = regulation_data.copy()\n",
    "hourly_regs['hour'] = hourly_regs['timestamp'].dt.hour\n",
    "hourly_reg_rate = hourly_regs.groupby('hour')['has_regulation'].mean()\n",
    "axes[1, 0].bar(hourly_reg_rate.index, hourly_reg_rate.values)\n",
    "axes[1, 0].set_title('Regulation Rate by Hour of Day')\n",
    "axes[1, 0].set_xlabel('Hour')\n",
    "axes[1, 0].set_ylabel('Regulation Rate')\n",
    "\n",
    "# Weather code distribution\n",
    "weather_counts = weather_data['weather_code'].value_counts()\n",
    "axes[1, 1].bar(weather_counts.index, weather_counts.values)\n",
    "axes[1, 1].set_title('Weather Code Distribution')\n",
    "axes[1, 1].set_xlabel('Weather Code')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Correlation heatmap\n",
    "numeric_cols = ['temperature', 'pressure', 'wind_speed', 'visibility', 'humidity']\n",
    "corr_data = weather_data[numeric_cols].corr()\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Weather Variable Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Weather Data Summary:\")\n",
    "print(weather_data[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing and Feature Engineering\n",
    "\n",
    "Now let's use the system's data pipeline to process and enhance our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.feature_engineering import WeatherFeatureEngineer, TimeSeriesFeatureEngineer\n",
    "from data.preprocessing import PreprocessingPipeline, TimeSeriesScaler\n",
    "\n",
    "# Create weather-specific features\n",
    "weather_engineer = WeatherFeatureEngineer()\n",
    "enhanced_weather = weather_engineer.create_features(weather_data)\n",
    "\n",
    "print(f\"Original features: {weather_data.shape[1]}\")\n",
    "print(f\"Enhanced features: {enhanced_weather.shape[1]}\")\n",
    "print(f\"New features added: {enhanced_weather.shape[1] - weather_data.shape[1]}\")\n",
    "\n",
    "# Show new features\n",
    "new_features = [col for col in enhanced_weather.columns if col not in weather_data.columns]\n",
    "print(f\"\\nNew features: {new_features}\")\n",
    "\n",
    "# Display sample of enhanced data\n",
    "print(\"\\nEnhanced weather data sample:\")\n",
    "print(enhanced_weather[['timestamp', 'temperature', 'flight_category', 'weather_severity', 'wind_components_u', 'wind_components_v']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series features\n",
    "ts_engineer = TimeSeriesFeatureEngineer()\n",
    "enhanced_data = ts_engineer.create_features(\n",
    "    enhanced_weather,\n",
    "    timestamp_col='timestamp',\n",
    "    value_cols=['temperature', 'pressure', 'wind_speed'],\n",
    "    lags=[1, 3, 6],  # 30min, 1.5h, 3h lags\n",
    "    rolling_windows=[6, 12]  # 3h, 6h rolling windows\n",
    ")\n",
    "\n",
    "print(f\"After time series engineering: {enhanced_data.shape[1]} features\")\n",
    "\n",
    "# Show some time series features\n",
    "ts_features = [col for col in enhanced_data.columns if 'lag_' in col or 'rolling_' in col or col in ['hour', 'day_of_week', 'month']]\n",
    "print(f\"\\nTime series features: {ts_features[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with regulation data to create final dataset\n",
    "final_data = enhanced_data.merge(\n",
    "    regulation_data[['timestamp', 'has_regulation']], \n",
    "    on='timestamp', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Remove rows with NaN (due to lag features)\n",
    "final_data = final_data.dropna()\n",
    "\n",
    "print(f\"Final dataset shape: {final_data.shape}\")\n",
    "print(f\"Regulation rate in final dataset: {final_data['has_regulation'].mean():.1%}\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in final_data.columns if col not in ['timestamp', 'airport', 'has_regulation', 'weather_code']]\n",
    "X = final_data[feature_cols]\n",
    "y = final_data['has_regulation']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Let's train different models using the system's training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from models.random_forest import RandomForestModel\n",
    "from models.fnn import FNNModel\n",
    "from training.trainer import Trainer\n",
    "from config import FNNConfig\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer()\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\n=== Training Random Forest ===\")\n",
    "rf_config = RandomForestConfig(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model = RandomForestModel(rf_config)\n",
    "\n",
    "rf_results = trainer.train_model(\n",
    "    model=rf_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    model_name=\"random_forest\"\n",
    ")\n",
    "\n",
    "print(f\"Random Forest Results:\")\n",
    "print(f\"- Accuracy: {rf_results['accuracy']:.3f}\")\n",
    "print(f\"- Precision: {rf_results['precision']:.3f}\")\n",
    "print(f\"- Recall: {rf_results['recall']:.3f}\")\n",
    "print(f\"- F1 Score: {rf_results['f1_score']:.3f}\")\n",
    "print(f\"- Training Time: {rf_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Feedforward Neural Network\n",
    "print(\"\\n=== Training Feedforward Neural Network ===\")\n",
    "fnn_config = FNNConfig(\n",
    "    hidden_layer_sizes=[100, 50],\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "fnn_model = FNNModel(fnn_config)\n",
    "\n",
    "fnn_results = trainer.train_model(\n",
    "    model=fnn_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    model_name=\"feedforward_nn\"\n",
    ")\n",
    "\n",
    "print(f\"Feedforward NN Results:\")\n",
    "print(f\"- Accuracy: {fnn_results['accuracy']:.3f}\")\n",
    "print(f\"- Precision: {fnn_results['precision']:.3f}\")\n",
    "print(f\"- Recall: {fnn_results['recall']:.3f}\")\n",
    "print(f\"- F1 Score: {fnn_results['f1_score']:.3f}\")\n",
    "print(f\"- Training Time: {fnn_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison\n",
    "\n",
    "Let's evaluate our models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "\n",
    "# Random Forest test evaluation\n",
    "rf_test_metrics = rf_model.evaluate(X_test, y_test)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_probabilities = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nRandom Forest Test Results:\")\n",
    "print(f\"- Accuracy: {rf_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {rf_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {rf_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {rf_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {rf_test_metrics.auc_roc:.3f}\")\n",
    "\n",
    "# FNN test evaluation\n",
    "fnn_test_metrics = fnn_model.evaluate(X_test, y_test)\n",
    "fnn_predictions = fnn_model.predict(X_test)\n",
    "fnn_probabilities = fnn_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nFeedforward NN Test Results:\")\n",
    "print(f\"- Accuracy: {fnn_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {fnn_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {fnn_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {fnn_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {fnn_test_metrics.auc_roc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "from visualization.plots import ModelVisualizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "visualizer = ModelVisualizer()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Confusion Matrices Comparison', fontsize=16)\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_predictions)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Random Forest')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# FNN confusion matrix\n",
    "fnn_cm = confusion_matrix(y_test, fnn_predictions)\n",
    "sns.heatmap(fnn_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Feedforward Neural Network')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Random Forest ROC\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probabilities[:, 1])\n",
    "rf_auc = auc(rf_fpr, rf_tpr)\n",
    "plt.plot(rf_fpr, rf_tpr, linewidth=2, label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "\n",
    "# FNN ROC\n",
    "fnn_fpr, fnn_tpr, _ = roc_curve(y_test, fnn_probabilities[:, 1])\n",
    "fnn_auc = auc(fnn_fpr, fnn_tpr)\n",
    "plt.plot(fnn_fpr, fnn_tpr, linewidth=2, label=f'Feedforward NN (AUC = {fnn_auc:.3f})')\n",
    "\n",
    "# Random baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (Random Forest)\n",
    "rf_importance = rf_model.get_feature_importance()\n",
    "\n",
    "if rf_importance is not None:\n",
    "    # Plot top 15 most important features\n",
    "    top_features = rf_importance.head(15)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(rf_importance.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Let's demonstrate hyperparameter tuning to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.hyperparameter_tuning import GridSearchTuner\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "# Create new model for tuning\n",
    "tuning_model = RandomForestModel(RandomForestConfig(random_state=42))\n",
    "\n",
    "# Perform grid search\n",
    "tuner = GridSearchTuner(scoring='f1', n_jobs=-1)\n",
    "tuning_result = tuner.tune(\n",
    "    model=tuning_model,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    cv=3  # 3-fold CV for speed\n",
    ")\n",
    "\n",
    "print(f\"\\nBest parameters: {tuning_result.best_params}\")\n",
    "print(f\"Best CV score: {tuning_result.best_score:.3f}\")\n",
    "print(f\"Number of trials: {len(tuning_result.all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with best parameters\n",
    "print(\"\\n=== Training Optimized Random Forest ===\")\n",
    "optimized_config = RandomForestConfig(**tuning_result.best_params, random_state=42)\n",
    "optimized_model = RandomForestModel(optimized_config)\n",
    "\n",
    "optimized_results = trainer.train_model(\n",
    "    model=optimized_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    model_name=\"optimized_random_forest\"\n",
    ")\n",
    "\n",
    "# Test the optimized model\n",
    "optimized_test_metrics = optimized_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nOptimized Random Forest Test Results:\")\n",
    "print(f\"- Accuracy: {optimized_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {optimized_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {optimized_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {optimized_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc:.3f}\")\n",
    "\n",
    "# Compare with original\n",
    "print(f\"\\nImprovement over original:\")\n",
    "print(f\"- Accuracy: {optimized_test_metrics.accuracy - rf_test_metrics.accuracy:+.3f}\")\n",
    "print(f\"- F1 Score: {optimized_test_metrics.f1_score - rf_test_metrics.f1_score:+.3f}\")\n",
    "print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc - rf_test_metrics.auc_roc:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Management and Reporting\n",
    "\n",
    "Let's save our results and generate a comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results.results_manager import ResultsManager, ExperimentResult, ModelResult\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results manager\n",
    "results_manager = ResultsManager(base_path=\"./tutorial_results\")\n",
    "\n",
    "# Create experiment result\n",
    "experiment_result = ExperimentResult(\n",
    "    experiment_id=\"tutorial_experiment_001\",\n",
    "    experiment_name=\"Quick Start Tutorial Experiment\",\n",
    "    timestamp=datetime.now(),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Create model results for each trained model\n",
    "models_data = [\n",
    "    (\"Random Forest\", rf_test_metrics, rf_results['training_time']),\n",
    "    (\"Feedforward NN\", fnn_test_metrics, fnn_results['training_time']),\n",
    "    (\"Optimized RF\", optimized_test_metrics, optimized_results['training_time'])\n",
    "]\n",
    "\n",
    "for model_name, metrics, training_time in models_data:\n",
    "    model_result = ModelResult(\n",
    "        model_name=model_name,\n",
    "        model_type=model_name.split()[0],  # First word as type\n",
    "        timestamp=datetime.now(),\n",
    "        config={},\n",
    "        training_time=training_time,\n",
    "        test_accuracy=metrics.accuracy,\n",
    "        test_precision=metrics.precision,\n",
    "        test_recall=metrics.recall,\n",
    "        test_f1=metrics.f1_score,\n",
    "        test_auc=metrics.auc_roc,\n",
    "        confusion_matrix=metrics.confusion_matrix\n",
    "    )\n",
    "    experiment_result.add_model_result(model_result)\n",
    "\n",
    "# Save experiment\n",
    "experiment_id = results_manager.save_experiment_result(experiment_result)\n",
    "print(f\"Experiment saved with ID: {experiment_id}\")\n",
    "\n",
    "# Display experiment summary\n",
    "print(f\"\\nExperiment Summary:\")\n",
    "print(f\"- Best Model: {experiment_result.best_model}\")\n",
    "print(f\"- Best Accuracy: {experiment_result.best_accuracy:.3f}\")\n",
    "print(f\"- Models Trained: {len(experiment_result.model_results)}\")\n",
    "print(f\"- Total Training Time: {experiment_result.total_training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "from results.report_generator import ReportGenerator\n",
    "\n",
    "# Create report generator\n",
    "report_generator = ReportGenerator()\n",
    "\n",
    "# Generate HTML report\n",
    "try:\n",
    "    html_report_path = report_generator.generate_report(\n",
    "        experiment=experiment_result,\n",
    "        format='html',\n",
    "        output_path='tutorial_experiment_report.html',\n",
    "        include_visualizations=True\n",
    "    )\n",
    "    print(f\"HTML report generated: {html_report_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate HTML report: {e}\")\n",
    "\n",
    "# Generate Markdown report\n",
    "try:\n",
    "    md_report_path = report_generator.generate_report(\n",
    "        experiment=experiment_result,\n",
    "        format='markdown',\n",
    "        output_path='tutorial_experiment_report.md'\n",
    "    )\n",
    "    print(f\"Markdown report generated: {md_report_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate Markdown report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Summary\n",
    "\n",
    "Let's create a final comparison of all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, model_result in experiment_result.model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': model_result.test_accuracy,\n",
    "        'Precision': model_result.test_precision,\n",
    "        'Recall': model_result.test_recall,\n",
    "        'F1 Score': model_result.test_f1,\n",
    "        'AUC-ROC': model_result.test_auc,\n",
    "        'Training Time (s)': model_result.training_time\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(3)\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "print(\"=\" * 40)\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_value = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"{metric:>12}: {best_model} ({best_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Model performance radar chart\n",
    "import math\n",
    "\n",
    "# Prepare data for radar chart\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "    values = [row[metric] for metric in metrics]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Comparison (Radar Chart)', size=16, y=1.1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed the quick start tutorial for the Weather Regulation Prediction System. Here's what you've learned:\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "1. **Data Generation**: Created realistic synthetic weather and regulation data\n",
    "2. **Feature Engineering**: Enhanced raw data with domain-specific and time series features\n",
    "3. **Model Training**: Trained multiple machine learning models (Random Forest, Neural Network)\n",
    "4. **Hyperparameter Tuning**: Optimized model performance using grid search\n",
    "5. **Evaluation**: Compared models using various metrics and visualizations\n",
    "6. **Results Management**: Saved and organized experimental results\n",
    "7. **Reporting**: Generated comprehensive reports in multiple formats\n",
    "\n",
    "### Key Insights from This Tutorial:\n",
    "\n",
    "- Weather features significantly impact regulation prediction accuracy\n",
    "- Time series features (lags, rolling statistics) improve model performance\n",
    "- Hyperparameter tuning can provide meaningful improvements\n",
    "- Different models excel at different aspects (precision vs recall trade-offs)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try More Models**: Experiment with LSTM, CNN, Transformer, or Ensemble models\n",
    "2. **Advanced Tuning**: Use Bayesian optimization for more efficient hyperparameter search\n",
    "3. **Real Data**: Apply the system to actual METAR and ATFM regulation data\n",
    "4. **Feature Engineering**: Explore additional weather-specific features\n",
    "5. **Cross-Validation**: Implement time series cross-validation for more robust evaluation\n",
    "6. **Production Deployment**: Set up model monitoring and retraining pipelines\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- Check out the other tutorial notebooks for advanced topics\n",
    "- Refer to the API documentation for detailed function references\n",
    "- Explore the configuration system for customizing experiments\n",
    "- Use the interactive dashboard for model comparison and analysis\n",
    "\n",
    "Thank you for using the Weather Regulation Prediction System!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
