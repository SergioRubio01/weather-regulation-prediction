{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Regulation Prediction System - Quick Start Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to the Weather Regulation Prediction System. You'll learn how to:\n",
    "\n",
    "1. Load and configure the system\n",
    "2. Prepare weather and regulation data\n",
    "3. Train different machine learning models\n",
    "4. Evaluate and compare model performance\n",
    "5. Generate reports and visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have installed all required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Configuration\n",
    "\n",
    "The system uses a configuration-based approach for managing experiments. Let's start by creating a basic configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with the Balanced Dataset\n",
    "\n",
    "In the original dataset, there's often a significant class imbalance where regulations are much less frequent than normal operations (typically <10% regulation rate). This imbalance can cause machine learning models to be biased toward predicting \"no regulation\" most of the time, leading to poor detection of actual regulation events.\n",
    "\n",
    "### Why Use a Balanced Dataset?\n",
    "\n",
    "1. **Improved Model Performance**: Balanced datasets help models learn features of both classes equally well\n",
    "2. **Better Recall**: Models trained on balanced data are better at detecting rare events (regulations)\n",
    "3. **Fair Evaluation**: Performance metrics are more meaningful when classes are balanced\n",
    "4. **Reduced Bias**: Models don't simply learn to predict the majority class\n",
    "\n",
    "Let's load the pre-prepared balanced dataset that has been created using SMOTE (Synthetic Minority Over-sampling Technique) to achieve a 50-50 class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load the pre-prepared balanced dataset\n",
    "import os\n",
    "\n",
    "balanced_data_path = '../data/balanced_weather_data.csv'\n",
    "\n",
    "if os.path.exists(balanced_data_path):\n",
    "    print(\"Loading pre-prepared balanced dataset...\")\n",
    "    balanced_data = pd.read_csv(balanced_data_path)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    balanced_data['timestamp'] = pd.to_datetime(balanced_data['timestamp'])\n",
    "    \n",
    "    print(f\"Balanced dataset shape: {balanced_data.shape}\")\n",
    "    print(f\"Regulation distribution:\")\n",
    "    print(balanced_data['has_regulation'].value_counts())\n",
    "    print(f\"Regulation rate: {balanced_data['has_regulation'].mean():.1%}\")\n",
    "    \n",
    "    # Use balanced data for the rest of the tutorial\n",
    "    use_balanced = True\n",
    "else:\n",
    "    print(\"Balanced dataset not found. We'll generate synthetic data instead.\")\n",
    "    use_balanced = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Data (Alternative)\n",
    "\n",
    "If the balanced dataset is not available, we'll generate synthetic weather and regulation data that resembles real aviation data. This section is skipped if we successfully loaded the balanced dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<cell_type>code</cell_type># Generate synthetic weather data (only if balanced dataset not available)\n",
    "if not use_balanced:\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "\n",
    "    # Create realistic weather patterns\n",
    "    timestamps = pd.date_range('2023-01-01', periods=n_samples, freq='30min')\n",
    "\n",
    "    # Temperature with daily cycle\n",
    "    hour_of_day = timestamps.hour\n",
    "    temp_base = 10 + 8 * np.sin(2 * np.pi * hour_of_day / 24)  # Daily temperature cycle\n",
    "    temperature = temp_base + np.random.normal(0, 3, n_samples)  # Add noise\n",
    "\n",
    "    # Pressure (realistic values)\n",
    "    pressure = np.random.normal(1013, 15, n_samples)\n",
    "\n",
    "    # Wind speed (typically lower at night)\n",
    "    wind_base = 5 + 3 * np.sin(2 * np.pi * hour_of_day / 24 + np.pi/2)\n",
    "    wind_speed = np.abs(wind_base + np.random.normal(0, 2, n_samples))\n",
    "\n",
    "    # Wind direction\n",
    "    wind_direction = np.random.uniform(0, 360, n_samples)\n",
    "\n",
    "    # Visibility (lower during poor weather)\n",
    "    visibility = np.random.lognormal(9.2, 0.5, n_samples)  # Log-normal distribution\n",
    "    visibility = np.clip(visibility, 1000, 20000)  # Realistic range\n",
    "\n",
    "    # Humidity\n",
    "    humidity = np.random.beta(3, 2, n_samples) * 100  # Beta distribution\n",
    "\n",
    "    # Weather codes (simplified)\n",
    "    weather_codes = np.random.choice(['CLR', 'FEW', 'SCT', 'BKN', 'OVC', 'RA', 'SN', 'FG'], \n",
    "                                    n_samples, p=[0.3, 0.2, 0.15, 0.15, 0.1, 0.05, 0.02, 0.03])\n",
    "\n",
    "    # Create weather DataFrame\n",
    "    weather_data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'airport': 'EGLL',\n",
    "        'temperature': temperature,\n",
    "        'pressure': pressure,\n",
    "        'wind_speed': wind_speed,\n",
    "        'wind_direction': wind_direction,\n",
    "        'visibility': visibility,\n",
    "        'humidity': humidity,\n",
    "        'weather_code': weather_codes\n",
    "    })\n",
    "\n",
    "    print(f\"Generated {len(weather_data)} weather observations\")\n",
    "    print(\"\\nWeather data sample:\")\n",
    "    print(weather_data.head())\n",
    "else:\n",
    "    # If using balanced dataset, extract weather features\n",
    "    weather_data = balanced_data\n",
    "    print(\"Using weather features from balanced dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regulation data (only if balanced dataset not available)\n",
    "if not use_balanced:\n",
    "    # Create weather severity score\n",
    "    weather_severity = (\n",
    "        (weather_data['visibility'] < 5000).astype(int) * 0.3 +  # Low visibility\n",
    "        (weather_data['wind_speed'] > 15).astype(int) * 0.2 +    # High wind\n",
    "        weather_data['weather_code'].isin(['RA', 'SN', 'FG']).astype(int) * 0.3 +  # Precipitation/fog\n",
    "        (weather_data['temperature'] < 0).astype(int) * 0.2      # Freezing conditions\n",
    "    )\n",
    "\n",
    "    # Probability of regulation based on weather severity\n",
    "    regulation_prob = 0.1 + 0.4 * weather_severity  # Base 10% + up to 40% for severe weather\n",
    "\n",
    "    # Generate regulations with some randomness\n",
    "    has_regulation = np.random.binomial(1, regulation_prob)\n",
    "\n",
    "    # Create regulation DataFrame\n",
    "    regulation_data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'airport': 'EGLL',\n",
    "        'has_regulation': has_regulation,\n",
    "        'regulation_type': np.where(has_regulation, \n",
    "                                   np.random.choice(['WX', 'ATC', 'EQ'], len(has_regulation)), \n",
    "                                   'None'),\n",
    "        'severity_score': weather_severity\n",
    "    })\n",
    "\n",
    "    regulation_rate = has_regulation.mean()\n",
    "    print(f\"Generated {len(regulation_data)} regulation records\")\n",
    "    print(f\"Regulation rate: {regulation_rate:.1%}\")\n",
    "    print(f\"Total regulations: {has_regulation.sum()}\")\n",
    "\n",
    "    print(\"\\nRegulation data sample:\")\n",
    "    print(regulation_data.head())\n",
    "    \n",
    "    # Merge weather and regulation data\n",
    "    final_data = weather_data.merge(regulation_data[['timestamp', 'has_regulation']], on='timestamp')\n",
    "else:\n",
    "    # For balanced dataset, regulations are already included\n",
    "    final_data = balanced_data\n",
    "    regulation_rate = final_data['has_regulation'].mean()\n",
    "    print(f\"Balanced dataset regulation rate: {regulation_rate:.1%}\")\n",
    "    print(f\"Total samples: {len(final_data)}\")\n",
    "    print(f\"Regulations: {final_data['has_regulation'].sum()}\")\n",
    "    print(f\"No regulations: {(1 - final_data['has_regulation']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Visualization\n",
    "\n",
    "Let's explore our data to understand the patterns. Notice how the balanced dataset provides equal representation of both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create visualizations of the data\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Weather Data Exploration', fontsize=16)\n\n# Show basic statistics first\nprint(\"Data Overview:\")\nif use_balanced:\n    print(f\"✅ Using balanced dataset with {len(weather_data)} samples\")\n    print(f\"Regulation rate: {weather_data['has_regulation'].mean():.1%}\")\nelse:\n    print(f\"Using synthetic dataset with {len(weather_data)} samples\")\n\n# For visualization, merge with regulation data if needed\nif use_balanced:\n    reg_data = weather_data\nelse:\n    reg_data = weather_data.merge(regulation_data[['timestamp', 'has_regulation']], on='timestamp')\n\n# Temperature over time (sample)\nsample_data = reg_data.head(200)  # Use first 200 points for clarity\naxes[0, 0].plot(sample_data['timestamp'], sample_data['temperature'])\naxes[0, 0].set_title('Temperature over Time (Sample)')\naxes[0, 0].set_ylabel('Temperature (°C)')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Pressure distribution\naxes[0, 1].hist(weather_data['pressure'], bins=30, alpha=0.7)\naxes[0, 1].set_title('Pressure Distribution')\naxes[0, 1].set_xlabel('Pressure (hPa)')\naxes[0, 1].set_ylabel('Frequency')\n\n# Wind speed vs regulations\nfor reg_status, label in [(0, 'No Regulation'), (1, 'Regulation')]:\n    data_subset = reg_data[reg_data['has_regulation'] == reg_status].sample(min(500, len(reg_data[reg_data['has_regulation'] == reg_status])))\n    axes[0, 2].scatter(data_subset['wind_speed'], data_subset['visibility'], \n                      alpha=0.6, label=label, s=20)\naxes[0, 2].set_title('Wind Speed vs Visibility (by Regulation Status)')\naxes[0, 2].set_xlabel('Wind Speed (kt)')\naxes[0, 2].set_ylabel('Visibility (m)')\naxes[0, 2].legend()\n\n# Regulations by hour of day (if timestamp has hour info)\nif 'timestamp' in reg_data.columns:\n    hourly_regs = reg_data.copy()\n    hourly_regs['hour'] = hourly_regs['timestamp'].dt.hour\n    hourly_reg_rate = hourly_regs.groupby('hour')['has_regulation'].mean()\n    axes[1, 0].bar(hourly_reg_rate.index, hourly_reg_rate.values)\n    axes[1, 0].set_title('Regulation Rate by Hour of Day')\n    axes[1, 0].set_xlabel('Hour')\n    axes[1, 0].set_ylabel('Regulation Rate')\n\n# Weather code distribution (if available)\nif 'weather_code' in weather_data.columns:\n    weather_counts = weather_data['weather_code'].value_counts()\n    axes[1, 1].bar(weather_counts.index, weather_counts.values)\n    axes[1, 1].set_title('Weather Code Distribution')\n    axes[1, 1].set_xlabel('Weather Code')\n    axes[1, 1].set_ylabel('Count')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n\n# Correlation heatmap\nnumeric_cols = [col for col in weather_data.columns if weather_data[col].dtype in ['float64', 'int64']][:5]\nif len(numeric_cols) > 1:\n    corr_data = weather_data[numeric_cols].corr()\n    sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n    axes[1, 2].set_title('Weather Variable Correlations')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nnumeric_cols = [col for col in weather_data.columns if weather_data[col].dtype in ['float64', 'int64']]\nif len(numeric_cols) > 0:\n    print(\"\\nWeather Data Summary:\")\n    print(weather_data[numeric_cols].describe())",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>## 5. Data Processing and Feature Engineering\n\nNow let's use the system's data pipeline to process and enhance our data with weather-specific and time series features."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from data.feature_engineering import WeatherFeatureEngineer, TimeSeriesFeatureEngineer\nfrom data.preprocessing import PreprocessingPipeline, TimeSeriesScaler\n\n# Create weather-specific features\nweather_engineer = WeatherFeatureEngineer()\nenhanced_weather = weather_engineer.create_features(weather_data)\n\nprint(f\"Original features: {weather_data.shape[1]}\")\nprint(f\"Enhanced features: {enhanced_weather.shape[1]}\")\nprint(f\"New features added: {enhanced_weather.shape[1] - weather_data.shape[1]}\")\n\n# Show new features\nnew_features = [col for col in enhanced_weather.columns if col not in weather_data.columns]\nprint(f\"\\nNew features: {new_features}\")\n\n# Display sample of enhanced data\ndisplay_cols = [col for col in ['timestamp', 'temperature', 'flight_category', 'weather_severity', 'wind_components_u', 'wind_components_v'] if col in enhanced_weather.columns]\nif len(display_cols) > 1:\n    print(\"\\nEnhanced weather data sample:\")\n    print(enhanced_weather[display_cols].head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<cell_type>code</cell_type># Create time series features\n",
    "ts_engineer = TimeSeriesFeatureEngineer()\n",
    "\n",
    "# For balanced dataset, we need to check which columns exist\n",
    "if use_balanced:\n",
    "    # Identify numeric columns (excluding target and categorical)\n",
    "    numeric_cols = [col for col in enhanced_weather.columns \n",
    "                   if col not in ['timestamp', 'airport', 'has_regulation', 'weather_code', 'flight_category'] \n",
    "                   and enhanced_weather[col].dtype in ['float64', 'int64']]\n",
    "    # Select a subset of important features for time series\n",
    "    value_cols = [col for col in ['temperature', 'pressure', 'wind_speed', 'visibility', 'humidity'] \n",
    "                  if col in numeric_cols]\n",
    "else:\n",
    "    value_cols = ['temperature', 'pressure', 'wind_speed']\n",
    "\n",
    "enhanced_data = ts_engineer.create_features(\n",
    "    enhanced_weather,\n",
    "    timestamp_col='timestamp',\n",
    "    value_cols=value_cols,\n",
    "    lags=[1, 3, 6],  # 30min, 1.5h, 3h lags (or appropriate for data frequency)\n",
    "    rolling_windows=[6, 12]  # 3h, 6h rolling windows\n",
    ")\n",
    "\n",
    "print(f\"After time series engineering: {enhanced_data.shape[1]} features\")\n",
    "\n",
    "# Show some time series features\n",
    "ts_features = [col for col in enhanced_data.columns if 'lag_' in col or 'rolling_' in col or col in ['hour', 'day_of_week', 'month']]\n",
    "print(f\"\\nTime series features: {ts_features[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Combine with regulation data to create final dataset\nif 'has_regulation' in enhanced_data.columns:\n    # For balanced dataset, regulation is already included\n    final_data = enhanced_data\nelse:\n    # For synthetic data, merge with regulation data\n    final_data = enhanced_data.merge(\n        regulation_data[['timestamp', 'has_regulation']], \n        on='timestamp', \n        how='inner'\n    )\n\n# Remove rows with NaN (due to lag features)\nfinal_data = final_data.dropna()\n\nprint(f\"Final dataset shape: {final_data.shape}\")\nprint(f\"Regulation rate in final dataset: {final_data['has_regulation'].mean():.1%}\")\n\n# Prepare features and target\nfeature_cols = [col for col in final_data.columns \n                if col not in ['timestamp', 'airport', 'has_regulation', 'weather_code', 'flight_category', 'regulation_type']]\nX = final_data[feature_cols]\ny = final_data['has_regulation']\n\nprint(f\"\\nFeature matrix shape: {X.shape}\")\nprint(f\"Target distribution: {y.value_counts().to_dict()}\")\n\n# If using balanced dataset, highlight the benefit\nif use_balanced:\n    print(\"\\n✅ Using balanced dataset - both classes are equally represented!\")\n    print(\"This helps the model learn patterns from both regulation and non-regulation cases equally.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>## 5. Model Training\n\nLet's train different models using the system's training pipeline. Notice how the balanced dataset helps both models achieve better performance."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom models.random_forest import RandomForestModel\nfrom models.fnn import FNNModel\nfrom training.trainer import Trainer\nfrom config import RandomForestConfig, FNNConfig\n\n# Split the data\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.4, random_state=42, stratify=y\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\n\n# Initialize trainer\ntrainer = Trainer()\n\n# Train Random Forest\nprint(\"\\n=== Training Random Forest ===\")\nrf_config = RandomForestConfig(n_estimators=100, max_depth=10, random_state=42)\nrf_model = RandomForestModel(rf_config)\n\nrf_results = trainer.train_model(\n    model=rf_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n    model_name=\"random_forest\"\n)\n\nprint(f\"Random Forest Results:\")\nprint(f\"- Accuracy: {rf_results['accuracy']:.3f}\")\nprint(f\"- Precision: {rf_results['precision']:.3f}\")\nprint(f\"- Recall: {rf_results['recall']:.3f}\")\nprint(f\"- F1 Score: {rf_results['f1_score']:.3f}\")\nprint(f\"- Training Time: {rf_results['training_time']:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Train Feedforward Neural Network\nprint(\"\\n=== Training Feedforward Neural Network ===\")\nfnn_config = FNNConfig(\n    hidden_layer_sizes=[100, 50],\n    max_iter=200,\n    random_state=42,\n    early_stopping=True\n)\nfnn_model = FNNModel(fnn_config)\n\nfnn_results = trainer.train_model(\n    model=fnn_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n    model_name=\"feedforward_nn\"\n)\n\nprint(f\"Feedforward NN Results:\")\nprint(f\"- Accuracy: {fnn_results['accuracy']:.3f}\")\nprint(f\"- Precision: {fnn_results['precision']:.3f}\")\nprint(f\"- Recall: {fnn_results['recall']:.3f}\")\nprint(f\"- F1 Score: {fnn_results['f1_score']:.3f}\")\nprint(f\"- Training Time: {fnn_results['training_time']:.2f}s\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "\n",
    "# Random Forest test evaluation\n",
    "rf_test_metrics = rf_model.evaluate(X_test, y_test)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_probabilities = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nRandom Forest Test Results:\")\n",
    "print(f\"- Accuracy: {rf_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {rf_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {rf_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {rf_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {rf_test_metrics.auc_roc:.3f}\")\n",
    "\n",
    "# FNN test evaluation\n",
    "fnn_test_metrics = fnn_model.evaluate(X_test, y_test)\n",
    "fnn_predictions = fnn_model.predict(X_test)\n",
    "fnn_probabilities = fnn_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nFeedforward NN Test Results:\")\n",
    "print(f\"- Accuracy: {fnn_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {fnn_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {fnn_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {fnn_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {fnn_test_metrics.auc_roc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "from visualization.plots import ModelVisualizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "visualizer = ModelVisualizer()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Confusion Matrices Comparison', fontsize=16)\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_predictions)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Random Forest')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# FNN confusion matrix\n",
    "fnn_cm = confusion_matrix(y_test, fnn_predictions)\n",
    "sns.heatmap(fnn_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Feedforward Neural Network')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Random Forest ROC\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probabilities[:, 1])\n",
    "rf_auc = auc(rf_fpr, rf_tpr)\n",
    "plt.plot(rf_fpr, rf_tpr, linewidth=2, label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "\n",
    "# FNN ROC\n",
    "fnn_fpr, fnn_tpr, _ = roc_curve(y_test, fnn_probabilities[:, 1])\n",
    "fnn_auc = auc(fnn_fpr, fnn_tpr)\n",
    "plt.plot(fnn_fpr, fnn_tpr, linewidth=2, label=f'Feedforward NN (AUC = {fnn_auc:.3f})')\n",
    "\n",
    "# Random baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (Random Forest)\n",
    "rf_importance = rf_model.get_feature_importance()\n",
    "\n",
    "if rf_importance is not None:\n",
    "    # Plot top 15 most important features\n",
    "    top_features = rf_importance.head(15)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(rf_importance.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Let's demonstrate hyperparameter tuning to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.hyperparameter_tuning import GridSearchTuner\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "# Create new model for tuning\n",
    "tuning_model = RandomForestModel(RandomForestConfig(random_state=42))\n",
    "\n",
    "# Perform grid search\n",
    "tuner = GridSearchTuner(scoring='f1', n_jobs=-1)\n",
    "tuning_result = tuner.tune(\n",
    "    model=tuning_model,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    cv=3  # 3-fold CV for speed\n",
    ")\n",
    "\n",
    "print(f\"\\nBest parameters: {tuning_result.best_params}\")\n",
    "print(f\"Best CV score: {tuning_result.best_score:.3f}\")\n",
    "print(f\"Number of trials: {len(tuning_result.all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with best parameters\n",
    "print(\"\\n=== Training Optimized Random Forest ===\")\n",
    "optimized_config = RandomForestConfig(**tuning_result.best_params, random_state=42)\n",
    "optimized_model = RandomForestModel(optimized_config)\n",
    "\n",
    "optimized_results = trainer.train_model(\n",
    "    model=optimized_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    model_name=\"optimized_random_forest\"\n",
    ")\n",
    "\n",
    "# Test the optimized model\n",
    "optimized_test_metrics = optimized_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nOptimized Random Forest Test Results:\")\n",
    "print(f\"- Accuracy: {optimized_test_metrics.accuracy:.3f}\")\n",
    "print(f\"- Precision: {optimized_test_metrics.precision:.3f}\")\n",
    "print(f\"- Recall: {optimized_test_metrics.recall:.3f}\")\n",
    "print(f\"- F1 Score: {optimized_test_metrics.f1_score:.3f}\")\n",
    "print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc:.3f}\")\n",
    "\n",
    "# Compare with original\n",
    "print(f\"\\nImprovement over original:\")\n",
    "print(f\"- Accuracy: {optimized_test_metrics.accuracy - rf_test_metrics.accuracy:+.3f}\")\n",
    "print(f\"- F1 Score: {optimized_test_metrics.f1_score - rf_test_metrics.f1_score:+.3f}\")\n",
    "print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc - rf_test_metrics.auc_roc:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Management and Reporting\n",
    "\n",
    "Let's save our results and generate a comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results.results_manager import ResultsManager, ExperimentResult, ModelResult\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results manager\n",
    "results_manager = ResultsManager(base_path=\"./tutorial_results\")\n",
    "\n",
    "# Create experiment result\n",
    "experiment_result = ExperimentResult(\n",
    "    experiment_id=\"tutorial_experiment_001\",\n",
    "    experiment_name=\"Quick Start Tutorial Experiment\",\n",
    "    timestamp=datetime.now(),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Create model results for each trained model\n",
    "models_data = [\n",
    "    (\"Random Forest\", rf_test_metrics, rf_results['training_time']),\n",
    "    (\"Feedforward NN\", fnn_test_metrics, fnn_results['training_time']),\n",
    "    (\"Optimized RF\", optimized_test_metrics, optimized_results['training_time'])\n",
    "]\n",
    "\n",
    "for model_name, metrics, training_time in models_data:\n",
    "    model_result = ModelResult(\n",
    "        model_name=model_name,\n",
    "        model_type=model_name.split()[0],  # First word as type\n",
    "        timestamp=datetime.now(),\n",
    "        config={},\n",
    "        training_time=training_time,\n",
    "        test_accuracy=metrics.accuracy,\n",
    "        test_precision=metrics.precision,\n",
    "        test_recall=metrics.recall,\n",
    "        test_f1=metrics.f1_score,\n",
    "        test_auc=metrics.auc_roc,\n",
    "        confusion_matrix=metrics.confusion_matrix\n",
    "    )\n",
    "    experiment_result.add_model_result(model_result)\n",
    "\n",
    "# Save experiment\n",
    "experiment_id = results_manager.save_experiment_result(experiment_result)\n",
    "print(f\"Experiment saved with ID: {experiment_id}\")\n",
    "\n",
    "# Display experiment summary\n",
    "print(f\"\\nExperiment Summary:\")\n",
    "print(f\"- Best Model: {experiment_result.best_model}\")\n",
    "print(f\"- Best Accuracy: {experiment_result.best_accuracy:.3f}\")\n",
    "print(f\"- Models Trained: {len(experiment_result.model_results)}\")\n",
    "print(f\"- Total Training Time: {experiment_result.total_training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "from results.report_generator import ReportGenerator\n",
    "\n",
    "# Create report generator\n",
    "report_generator = ReportGenerator()\n",
    "\n",
    "# Generate HTML report\n",
    "try:\n",
    "    html_report_path = report_generator.generate_report(\n",
    "        experiment=experiment_result,\n",
    "        format='html',\n",
    "        output_path='tutorial_experiment_report.html',\n",
    "        include_visualizations=True\n",
    "    )\n",
    "    print(f\"HTML report generated: {html_report_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate HTML report: {e}\")\n",
    "\n",
    "# Generate Markdown report\n",
    "try:\n",
    "    md_report_path = report_generator.generate_report(\n",
    "        experiment=experiment_result,\n",
    "        format='markdown',\n",
    "        output_path='tutorial_experiment_report.md'\n",
    "    )\n",
    "    print(f\"Markdown report generated: {md_report_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate Markdown report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Summary\n",
    "\n",
    "Let's create a final comparison of all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, model_result in experiment_result.model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': model_result.test_accuracy,\n",
    "        'Precision': model_result.test_precision,\n",
    "        'Recall': model_result.test_recall,\n",
    "        'F1 Score': model_result.test_f1,\n",
    "        'AUC-ROC': model_result.test_auc,\n",
    "        'Training Time (s)': model_result.training_time\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(3)\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "print(\"=\" * 40)\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_value = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"{metric:>12}: {best_model} ({best_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Model performance radar chart\n",
    "import math\n",
    "\n",
    "# Prepare data for radar chart\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "    values = [row[metric] for metric in metrics]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Comparison (Radar Chart)', size=16, y=1.1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 10. Benefits of Balanced Dataset Training\n\nLet's demonstrate the benefits of training on a balanced dataset by comparing it with an imbalanced scenario. This comparison shows why balanced datasets are crucial for weather regulation prediction.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of balanced vs imbalanced training\n",
    "if use_balanced:\n",
    "    print(\"=== Balanced Dataset Benefits ===\\n\")\n",
    "    \n",
    "    # Create an imbalanced subset for comparison\n",
    "    # Simulate typical real-world imbalance (10% regulation rate)\n",
    "    n_minority = int(0.1 * len(X_train))\n",
    "    n_majority = len(X_train) - n_minority\n",
    "    \n",
    "    # Get indices for each class\n",
    "    minority_indices = y_train[y_train == 1].index[:n_minority]\n",
    "    majority_indices = y_train[y_train == 0].index[:n_majority]\n",
    "    \n",
    "    # Create imbalanced dataset\n",
    "    imbalanced_indices = minority_indices.union(majority_indices)\n",
    "    X_train_imbalanced = X_train.loc[imbalanced_indices]\n",
    "    y_train_imbalanced = y_train.loc[imbalanced_indices]\n",
    "    \n",
    "    print(f\"Imbalanced training set: {y_train_imbalanced.value_counts().to_dict()}\")\n",
    "    print(f\"Imbalanced regulation rate: {y_train_imbalanced.mean():.1%}\")\n",
    "    \n",
    "    # Train model on imbalanced data\n",
    "    print(\"\\nTraining Random Forest on IMBALANCED data...\")\n",
    "    rf_imbalanced = RandomForestModel(rf_config)\n",
    "    rf_imbalanced_results = trainer.train_model(\n",
    "        model=rf_imbalanced,\n",
    "        X_train=X_train_imbalanced,\n",
    "        y_train=y_train_imbalanced,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        model_name=\"rf_imbalanced\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    rf_imbalanced_metrics = rf_imbalanced.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n📊 Balanced vs Imbalanced Model Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Metric':<20} {'Balanced':<15} {'Imbalanced':<15} {'Improvement':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    metrics_comparison = [\n",
    "        ('Accuracy', rf_test_metrics.accuracy, rf_imbalanced_metrics.accuracy),\n",
    "        ('Precision', rf_test_metrics.precision, rf_imbalanced_metrics.precision),\n",
    "        ('Recall', rf_test_metrics.recall, rf_imbalanced_metrics.recall),\n",
    "        ('F1 Score', rf_test_metrics.f1_score, rf_imbalanced_metrics.f1_score),\n",
    "        ('AUC-ROC', rf_test_metrics.auc_roc, rf_imbalanced_metrics.auc_roc)\n",
    "    ]\n",
    "    \n",
    "    for metric_name, balanced_val, imbalanced_val in metrics_comparison:\n",
    "        improvement = ((balanced_val - imbalanced_val) / imbalanced_val) * 100 if imbalanced_val > 0 else 0\n",
    "        print(f\"{metric_name:<20} {balanced_val:<15.3f} {imbalanced_val:<15.3f} {improvement:+14.1f}%\")\n",
    "    \n",
    "    print(\"\\n💡 Key Insights:\")\n",
    "    print(\"- Balanced training significantly improves RECALL (ability to detect regulations)\")\n",
    "    print(\"- F1 Score improvement shows better balance between precision and recall\")\n",
    "    print(\"- AUC-ROC improvement indicates better overall discrimination ability\")\n",
    "    \n",
    "    # Visualize prediction distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Balanced model predictions\n",
    "    balanced_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "    axes[0].hist(balanced_probs[y_test == 0], bins=30, alpha=0.7, label='No Regulation', color='blue')\n",
    "    axes[0].hist(balanced_probs[y_test == 1], bins=30, alpha=0.7, label='Regulation', color='red')\n",
    "    axes[0].set_title('Balanced Model - Prediction Probabilities')\n",
    "    axes[0].set_xlabel('Predicted Probability of Regulation')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Imbalanced model predictions\n",
    "    imbalanced_probs = rf_imbalanced.predict_proba(X_test)[:, 1]\n",
    "    axes[1].hist(imbalanced_probs[y_test == 0], bins=30, alpha=0.7, label='No Regulation', color='blue')\n",
    "    axes[1].hist(imbalanced_probs[y_test == 1], bins=30, alpha=0.7, label='Regulation', color='red')\n",
    "    axes[1].set_title('Imbalanced Model - Prediction Probabilities')\n",
    "    axes[1].set_xlabel('Predicted Probability of Regulation')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📌 Notice how the balanced model provides better separation between classes!\")\n",
    "    \n",
    "else:\n",
    "    print(\"This demonstration requires the balanced dataset to show the comparison.\")\n",
    "    print(\"When working with real weather data, using balanced datasets is crucial for:\")\n",
    "    print(\"- Detecting rare but important regulation events\")\n",
    "    print(\"- Achieving better recall without sacrificing precision\")\n",
    "    print(\"- Creating more reliable models for operational use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 11. Conclusion and Next Steps\n\nCongratulations! You've successfully completed the quick start tutorial for the Weather Regulation Prediction System. Here's what you've learned:\n\n### What You've Accomplished:\n\n1. **Balanced Dataset Usage**: Loaded and utilized a balanced dataset for improved model performance\n2. **Data Processing**: Enhanced raw data with domain-specific and time series features\n3. **Model Training**: Trained multiple machine learning models (Random Forest, Neural Network)\n4. **Hyperparameter Tuning**: Optimized model performance using grid search\n5. **Performance Comparison**: Demonstrated the critical importance of balanced training data\n6. **Results Management**: Saved and organized experimental results\n7. **Comprehensive Evaluation**: Compared models using various metrics and visualizations\n\n### Key Insights from This Tutorial:\n\n- **Balanced datasets are crucial** for detecting rare but important regulation events\n- Weather features significantly impact regulation prediction accuracy\n- Time series features (lags, rolling statistics) improve model performance\n- Hyperparameter tuning can provide meaningful improvements\n- Different models excel at different aspects (precision vs recall trade-offs)\n\n### Why Balanced Datasets Matter:\n\n- **Improved Recall**: Better detection of regulation events (critical for aviation safety)\n- **Fair Learning**: Both classes receive equal attention during training\n- **Better Calibration**: More reliable probability predictions\n- **Operational Reliability**: Essential for real-world aviation applications\n\n### Next Steps:\n\n1. **Try More Models**: Experiment with LSTM, CNN, Transformer, or Ensemble models\n2. **Advanced Tuning**: Use Bayesian optimization for more efficient hyperparameter search\n3. **Real Data**: Apply the system to actual METAR and ATFM regulation data\n4. **Feature Engineering**: Explore additional weather-specific features\n5. **Cross-Validation**: Implement time series cross-validation for more robust evaluation\n6. **Production Deployment**: Set up model monitoring and retraining pipelines\n\n### Additional Resources:\n\n- Check out the Advanced Deep Learning notebook for neural network models\n- Refer to the API documentation for detailed function references\n- Explore the configuration system for customizing experiments\n- Use the interactive dashboard for model comparison and analysis\n\nThank you for using the Weather Regulation Prediction System!",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}