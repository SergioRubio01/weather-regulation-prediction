{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Models for Weather Regulation Prediction\n",
    "\n",
    "This notebook demonstrates the advanced deep learning capabilities of the Weather Regulation Prediction System, including:\n",
    "\n",
    "1. **LSTM Networks** - For sequential weather pattern analysis\n",
    "2. **Transformer Models** - State-of-the-art attention-based architectures\n",
    "3. **CNN Models** - For spatial weather pattern recognition\n",
    "4. **Attention-LSTM** - Combining LSTM with attention mechanisms\n",
    "5. **Autoencoder** - For unsupervised feature learning\n",
    "6. **Ensemble Methods** - Combining multiple deep learning models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have TensorFlow installed:\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check TensorFlow availability\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. Some models will be skipped.\")\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Deep Learning\n",
    "\n",
    "Deep learning models require specific data preparation, especially for sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate more complex time series data for deep learning\n",
    "def generate_complex_weather_sequences(n_samples=2000, sequence_length=24):\n",
    "    \"\"\"Generate complex weather time series with multiple patterns\"\"\"\n",
    "    \n",
    "    # Time features\n",
    "    timestamps = pd.date_range('2023-01-01', periods=n_samples, freq='1H')\n",
    "    hour_of_day = timestamps.hour\n",
    "    day_of_year = timestamps.dayofyear\n",
    "    \n",
    "    # Multiple weather patterns\n",
    "    # 1. Diurnal temperature cycle\n",
    "    temp_diurnal = 15 + 10 * np.sin(2 * np.pi * hour_of_day / 24)\n",
    "    \n",
    "    # 2. Seasonal temperature cycle\n",
    "    temp_seasonal = 8 * np.sin(2 * np.pi * day_of_year / 365)\n",
    "    \n",
    "    # 3. Weather system movements (3-day cycles)\n",
    "    temp_systems = 5 * np.sin(2 * np.pi * np.arange(n_samples) / (3*24))\n",
    "    \n",
    "    # Combine temperature patterns with noise\n",
    "    temperature = temp_diurnal + temp_seasonal + temp_systems + np.random.normal(0, 2, n_samples)\n",
    "    \n",
    "    # Pressure anticorrelated with temperature systems\n",
    "    pressure = 1013 - 0.5 * temp_systems + np.random.normal(0, 8, n_samples)\n",
    "    \n",
    "    # Wind patterns\n",
    "    wind_base = 8 + 5 * np.sin(2 * np.pi * hour_of_day / 24 + np.pi/3)\n",
    "    wind_speed = np.abs(wind_base + 3 * np.sin(2 * np.pi * np.arange(n_samples) / (2*24)) + \n",
    "                        np.random.normal(0, 2, n_samples))\n",
    "    \n",
    "    wind_direction = (180 + 60 * np.sin(2 * np.pi * np.arange(n_samples) / (4*24)) + \n",
    "                     np.random.normal(0, 30, n_samples)) % 360\n",
    "    \n",
    "    # Visibility affected by humidity and weather conditions\n",
    "    humidity = 50 + 30 * np.sin(2 * np.pi * hour_of_day / 24 + np.pi) + np.random.normal(0, 10, n_samples)\n",
    "    humidity = np.clip(humidity, 10, 100)\n",
    "    \n",
    "    # Visibility decreases with high humidity and adverse weather\n",
    "    visibility_base = 15000 - 100 * humidity\n",
    "    visibility = np.clip(visibility_base + np.random.normal(0, 2000, n_samples), 1000, 20000)\n",
    "    \n",
    "    # Cloud cover patterns\n",
    "    cloud_cover = np.clip(humidity / 2 + np.random.normal(0, 20, n_samples), 0, 100)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'temperature': temperature,\n",
    "        'pressure': pressure,\n",
    "        'wind_speed': wind_speed,\n",
    "        'wind_direction': wind_direction,\n",
    "        'visibility': visibility,\n",
    "        'humidity': humidity,\n",
    "        'cloud_cover': cloud_cover,\n",
    "        'hour': hour_of_day,\n",
    "        'day_of_year': day_of_year\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "weather_data = generate_complex_weather_sequences(n_samples=2000)\n",
    "print(f\"Generated weather data shape: {weather_data.shape}\")\n",
    "print(\"\\nWeather data sample:\")\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regulation data with complex patterns\n",
    "def generate_regulation_patterns(weather_data):\n",
    "    \"\"\"Generate regulations based on complex weather interactions\"\"\"\n",
    "    \n",
    "    # Complex weather severity calculation\n",
    "    severity_components = {\n",
    "        'visibility': (weather_data['visibility'] < 5000).astype(float) * 0.4,\n",
    "        'wind': (weather_data['wind_speed'] > 20).astype(float) * 0.3,\n",
    "        'temperature': (weather_data['temperature'] < -2).astype(float) * 0.2,\n",
    "        'pressure_drop': (weather_data['pressure'].diff() < -2).astype(float) * 0.3,\n",
    "        'cloud_cover': (weather_data['cloud_cover'] > 80).astype(float) * 0.2,\n",
    "        'humidity': (weather_data['humidity'] > 85).astype(float) * 0.15\n",
    "    }\n",
    "    \n",
    "    # Combine severity components\n",
    "    weather_severity = sum(severity_components.values())\n",
    "    weather_severity = np.clip(weather_severity, 0, 1)\n",
    "    \n",
    "    # Time-based regulation probability (higher during peak hours)\n",
    "    peak_hours = weather_data['hour'].isin([6, 7, 8, 17, 18, 19])\n",
    "    time_factor = 1.3 * peak_hours + 0.8 * (~peak_hours)\n",
    "    \n",
    "    # Night operations have different thresholds\n",
    "    night_hours = weather_data['hour'].isin([22, 23, 0, 1, 2, 3, 4, 5])\n",
    "    night_factor = 1.5 * night_hours + 1.0 * (~night_hours)\n",
    "    \n",
    "    # Final regulation probability\n",
    "    base_prob = 0.08  # 8% base regulation rate\n",
    "    regulation_prob = base_prob + 0.35 * weather_severity * time_factor * night_factor\n",
    "    regulation_prob = np.clip(regulation_prob, 0, 0.8)\n",
    "    \n",
    "    # Generate actual regulations\n",
    "    has_regulation = np.random.binomial(1, regulation_prob)\n",
    "    \n",
    "    # Regulation types based on conditions\n",
    "    regulation_types = []\n",
    "    for i, reg in enumerate(has_regulation):\n",
    "        if reg == 0:\n",
    "            regulation_types.append('None')\n",
    "        else:\n",
    "            if weather_data.iloc[i]['visibility'] < 2000:\n",
    "                regulation_types.append('WX_VIS')\n",
    "            elif weather_data.iloc[i]['wind_speed'] > 25:\n",
    "                regulation_types.append('WX_WIND')\n",
    "            elif weather_data.iloc[i]['temperature'] < -5:\n",
    "                regulation_types.append('WX_ICE')\n",
    "            else:\n",
    "                regulation_types.append(np.random.choice(['WX', 'ATC', 'EQ'], p=[0.6, 0.3, 0.1]))\n",
    "    \n",
    "    return has_regulation, regulation_types, weather_severity\n",
    "\n",
    "# Generate regulations\n",
    "has_regulation, regulation_types, weather_severity = generate_regulation_patterns(weather_data)\n",
    "\n",
    "# Add to weather data\n",
    "weather_data['has_regulation'] = has_regulation\n",
    "weather_data['regulation_type'] = regulation_types\n",
    "weather_data['weather_severity'] = weather_severity\n",
    "\n",
    "regulation_rate = has_regulation.mean()\n",
    "print(f\"Regulation rate: {regulation_rate:.1%}\")\n",
    "print(f\"Total regulations: {has_regulation.sum()}\")\n",
    "\n",
    "# Show regulation distribution by type\n",
    "reg_type_counts = pd.Series(regulation_types).value_counts()\n",
    "print(\"\\nRegulation types:\")\n",
    "print(reg_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complex patterns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig.suptitle('Complex Weather and Regulation Patterns', fontsize=16)\n",
    "\n",
    "# Temperature patterns over time\n",
    "sample_data = weather_data.iloc[:168]  # First week\n",
    "axes[0, 0].plot(sample_data['timestamp'], sample_data['temperature'])\n",
    "axes[0, 0].set_title('Temperature Patterns (First Week)')\n",
    "axes[0, 0].set_ylabel('Temperature (Â°C)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pressure vs regulation correlation\n",
    "reg_data = weather_data[weather_data['has_regulation'] == 1]\n",
    "no_reg_data = weather_data[weather_data['has_regulation'] == 0].sample(len(reg_data))  # Sample for balance\n",
    "\n",
    "axes[0, 1].scatter(no_reg_data['pressure'], no_reg_data['visibility'], \n",
    "                  alpha=0.6, label='No Regulation', s=10)\n",
    "axes[0, 1].scatter(reg_data['pressure'], reg_data['visibility'], \n",
    "                  alpha=0.8, label='Regulation', s=10, color='red')\n",
    "axes[0, 1].set_title('Pressure vs Visibility by Regulation Status')\n",
    "axes[0, 1].set_xlabel('Pressure (hPa)')\n",
    "axes[0, 1].set_ylabel('Visibility (m)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Weather severity distribution\n",
    "axes[1, 0].hist(weather_data['weather_severity'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Weather Severity Distribution')\n",
    "axes[1, 0].set_xlabel('Weather Severity Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Regulation rate by hour\n",
    "hourly_reg_rate = weather_data.groupby('hour')['has_regulation'].mean()\n",
    "axes[1, 1].bar(hourly_reg_rate.index, hourly_reg_rate.values, color='skyblue', edgecolor='navy')\n",
    "axes[1, 1].set_title('Regulation Rate by Hour of Day')\n",
    "axes[1, 1].set_xlabel('Hour')\n",
    "axes[1, 1].set_ylabel('Regulation Rate')\n",
    "\n",
    "# Wind speed patterns\n",
    "axes[2, 0].plot(sample_data['timestamp'], sample_data['wind_speed'], label='Wind Speed', color='green')\n",
    "axes[2, 0].axhline(y=20, color='red', linestyle='--', label='High Wind Threshold')\n",
    "axes[2, 0].set_title('Wind Speed Patterns (First Week)')\n",
    "axes[2, 0].set_ylabel('Wind Speed (kt)')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_features = ['temperature', 'pressure', 'wind_speed', 'visibility', 'humidity', 'weather_severity']\n",
    "corr_matrix = weather_data[corr_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequence Data Preparation\n",
    "\n",
    "Deep learning models often work with sequences. Let's prepare our data for sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length=24, target_col='has_regulation'):\n",
    "    \"\"\"Create sequences for deep learning models\"\"\"\n",
    "    \n",
    "    # Select features for sequences\n",
    "    feature_cols = ['temperature', 'pressure', 'wind_speed', 'wind_direction', \n",
    "                   'visibility', 'humidity', 'cloud_cover', 'weather_severity']\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        # Get sequence of features\n",
    "        sequence = data[feature_cols].iloc[i-sequence_length:i].values\n",
    "        target = data[target_col].iloc[i]\n",
    "        \n",
    "        X_sequences.append(sequence)\n",
    "        y_sequences.append(target)\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_sequences), feature_cols\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 24  # 24 hours of history\n",
    "X_sequences, y_sequences, feature_names = create_sequences(weather_data, sequence_length)\n",
    "\n",
    "print(f\"Sequence data shape: {X_sequences.shape}\")\n",
    "print(f\"Target shape: {y_sequences.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Regulation rate in sequences: {y_sequences.mean():.1%}\")\n",
    "\n",
    "# Split data for deep learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split sequences\n",
    "X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.4, random_state=42, stratify=y_sequences\n",
    ")\n",
    "X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(\n",
    "    X_temp_seq, y_temp_seq, test_size=0.5, random_state=42, stratify=y_temp_seq\n",
    ")\n",
    "\n",
    "# Scale the sequences\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "X_train_reshaped = X_train_seq.reshape(-1, X_train_seq.shape[-1])\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_train_seq_scaled = X_train_scaled.reshape(X_train_seq.shape)\n",
    "\n",
    "X_val_reshaped = X_val_seq.reshape(-1, X_val_seq.shape[-1])\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "X_val_seq_scaled = X_val_scaled.reshape(X_val_seq.shape)\n",
    "\n",
    "X_test_reshaped = X_test_seq.reshape(-1, X_test_seq.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "X_test_seq_scaled = X_test_scaled.reshape(X_test_seq.shape)\n",
    "\n",
    "print(f\"\\nTraining sequences: {X_train_seq_scaled.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq_scaled.shape}\")\n",
    "print(f\"Test sequences: {X_test_seq_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Model Training\n",
    "\n",
    "Let's start with LSTM networks for sequential weather pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.lstm import LSTMModel\n",
    "    from config import LSTMConfig\n",
    "    from training.trainer import Trainer\n",
    "    \n",
    "    # Configure LSTM model\n",
    "    lstm_config = LSTMConfig(\n",
    "        units=64,\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.3,\n",
    "        bidirectional=True,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        sequence_length=sequence_length,\n",
    "        learning_rate=0.001,\n",
    "        early_stopping_patience=10\n",
    "    )\n",
    "    \n",
    "    print(\"Training LSTM model...\")\n",
    "    print(f\"Configuration: {lstm_config.__dict__}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    lstm_model = LSTMModel(lstm_config)\n",
    "    trainer = Trainer()\n",
    "    \n",
    "    lstm_results = trainer.train_model(\n",
    "        model=lstm_model,\n",
    "        X_train=X_train_seq_scaled,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_seq_scaled,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"lstm_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLSTM Training Results:\")\n",
    "    print(f\"- Accuracy: {lstm_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {lstm_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {lstm_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {lstm_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {lstm_results['training_time']:.2f}s\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping LSTM training\")\n",
    "    lstm_model = None\n",
    "    lstm_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM training history if available\n",
    "if TENSORFLOW_AVAILABLE and lstm_model is not None:\n",
    "    # Get training history\n",
    "    if hasattr(lstm_model, 'training_history') and lstm_model.training_history:\n",
    "        history = lstm_model.training_history\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss curves\n",
    "        if 'loss' in history:\n",
    "            axes[0].plot(history['loss'], label='Training Loss')\n",
    "            if 'val_loss' in history:\n",
    "                axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "            axes[0].set_title('LSTM Training Loss')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        if 'accuracy' in history:\n",
    "            axes[1].plot(history['accuracy'], label='Training Accuracy')\n",
    "            if 'val_accuracy' in history:\n",
    "                axes[1].plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "            axes[1].set_title('LSTM Training Accuracy')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Training history not available\")\n",
    "        \n",
    "    # Evaluate on test set\n",
    "    lstm_test_metrics = lstm_model.evaluate(X_test_seq_scaled, y_test_seq)\n",
    "    print(f\"\\nLSTM Test Results:\")\n",
    "    print(f\"- Accuracy: {lstm_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {lstm_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {lstm_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {lstm_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {lstm_test_metrics.auc_roc:.3f}\")\n",
    "else:\n",
    "    print(\"LSTM model not available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Model\n",
    "\n",
    "Now let's train a Transformer model for weather sequence analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.transformer import TransformerModel\n",
    "    from config import TransformerConfig\n",
    "    \n",
    "    # Configure Transformer model\n",
    "    transformer_config = TransformerConfig(\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        num_layers=4,\n",
    "        dropout=0.1,\n",
    "        sequence_length=sequence_length,\n",
    "        batch_size=32,\n",
    "        epochs=30,  # Fewer epochs for demo\n",
    "        learning_rate=0.0001,\n",
    "        warmup_steps=1000\n",
    "    )\n",
    "    \n",
    "    print(\"Training Transformer model...\")\n",
    "    print(f\"Configuration: {transformer_config.__dict__}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    transformer_model = TransformerModel(transformer_config)\n",
    "    \n",
    "    transformer_results = trainer.train_model(\n",
    "        model=transformer_model,\n",
    "        X_train=X_train_seq_scaled,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_seq_scaled,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"transformer_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTransformer Training Results:\")\n",
    "    print(f\"- Accuracy: {transformer_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {transformer_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {transformer_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {transformer_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {transformer_results['training_time']:.2f}s\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    transformer_test_metrics = transformer_model.evaluate(X_test_seq_scaled, y_test_seq)\n",
    "    print(f\"\\nTransformer Test Results:\")\n",
    "    print(f\"- Accuracy: {transformer_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {transformer_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {transformer_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {transformer_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {transformer_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping Transformer training\")\n",
    "    transformer_model = None\n",
    "    transformer_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention-LSTM Model\n",
    "\n",
    "Let's try an LSTM with attention mechanism for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.attention_lstm import AttentionLSTMModel\n",
    "    from config import LSTMConfig  # Reuse LSTM config for attention model\n",
    "    \n",
    "    # Configure Attention-LSTM model\n",
    "    attention_lstm_config = LSTMConfig(\n",
    "        units=64,\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.3,\n",
    "        batch_size=32,\n",
    "        epochs=40,\n",
    "        sequence_length=sequence_length,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(\"Training Attention-LSTM model...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    attention_lstm_model = AttentionLSTMModel(attention_lstm_config)\n",
    "    \n",
    "    attention_lstm_results = trainer.train_model(\n",
    "        model=attention_lstm_model,\n",
    "        X_train=X_train_seq_scaled,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_seq_scaled,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"attention_lstm_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAttention-LSTM Training Results:\")\n",
    "    print(f\"- Accuracy: {attention_lstm_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {attention_lstm_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {attention_lstm_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {attention_lstm_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {attention_lstm_results['training_time']:.2f}s\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    attention_lstm_test_metrics = attention_lstm_model.evaluate(X_test_seq_scaled, y_test_seq)\n",
    "    print(f\"\\nAttention-LSTM Test Results:\")\n",
    "    print(f\"- Accuracy: {attention_lstm_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {attention_lstm_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {attention_lstm_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {attention_lstm_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {attention_lstm_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping Attention-LSTM training\")\n",
    "    attention_lstm_model = None\n",
    "    attention_lstm_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CNN Model for Spatial Patterns\n",
    "\n",
    "CNNs can capture spatial patterns in weather data when reshaped appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.cnn import CNNModel\n",
    "    from config import CNNConfig\n",
    "    \n",
    "    # Reshape sequence data for CNN (treat sequence as 2D image)\n",
    "    # For CNN: (samples, height, width, channels)\n",
    "    # We'll reshape to treat time as height and features as width\n",
    "    def reshape_for_cnn(X_seq):\n",
    "        # X_seq shape: (samples, time_steps, features)\n",
    "        # Reshape to: (samples, time_steps, features, 1)\n",
    "        return np.expand_dims(X_seq, axis=-1)\n",
    "    \n",
    "    X_train_cnn = reshape_for_cnn(X_train_seq_scaled)\n",
    "    X_val_cnn = reshape_for_cnn(X_val_seq_scaled)\n",
    "    X_test_cnn = reshape_for_cnn(X_test_seq_scaled)\n",
    "    \n",
    "    print(f\"CNN input shape: {X_train_cnn.shape}\")\n",
    "    \n",
    "    # Configure CNN model\n",
    "    cnn_config = CNNConfig(\n",
    "        filters=[32, 64, 128],\n",
    "        kernel_sizes=[3, 3, 3],\n",
    "        pool_sizes=[2, 2, 2],\n",
    "        dropout=0.3,\n",
    "        batch_size=32,\n",
    "        epochs=30,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(\"Training CNN model...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    cnn_model = CNNModel(cnn_config)\n",
    "    \n",
    "    cnn_results = trainer.train_model(\n",
    "        model=cnn_model,\n",
    "        X_train=X_train_cnn,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_cnn,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"cnn_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCNN Training Results:\")\n",
    "    print(f\"- Accuracy: {cnn_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {cnn_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {cnn_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {cnn_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {cnn_results['training_time']:.2f}s\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    cnn_test_metrics = cnn_model.evaluate(X_test_cnn, y_test_seq)\n",
    "    print(f\"\\nCNN Test Results:\")\n",
    "    print(f\"- Accuracy: {cnn_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {cnn_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {cnn_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {cnn_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {cnn_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping CNN training\")\n",
    "    cnn_model = None\n",
    "    cnn_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Autoencoder for Feature Learning\n",
    "\n",
    "Let's use an autoencoder to learn compressed representations of weather patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.autoencoder import AutoencoderModel\n",
    "    from config import AutoencoderConfig\n",
    "    \n",
    "    # Flatten sequences for autoencoder\n",
    "    X_train_flat = X_train_seq_scaled.reshape(X_train_seq_scaled.shape[0], -1)\n",
    "    X_val_flat = X_val_seq_scaled.reshape(X_val_seq_scaled.shape[0], -1)\n",
    "    X_test_flat = X_test_seq_scaled.reshape(X_test_seq_scaled.shape[0], -1)\n",
    "    \n",
    "    print(f\"Flattened input shape: {X_train_flat.shape}\")\n",
    "    \n",
    "    # Configure Autoencoder\n",
    "    autoencoder_config = AutoencoderConfig(\n",
    "        encoding_dims=[128, 64, 32],  # Compress to 32 dimensions\n",
    "        dropout=0.2,\n",
    "        batch_size=32,\n",
    "        pretrain_epochs=20,  # Unsupervised pretraining\n",
    "        epochs=30,  # Supervised fine-tuning\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(\"Training Autoencoder model...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    autoencoder_model = AutoencoderModel(autoencoder_config)\n",
    "    \n",
    "    autoencoder_results = trainer.train_model(\n",
    "        model=autoencoder_model,\n",
    "        X_train=X_train_flat,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_flat,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"autoencoder_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAutoencoder Training Results:\")\n",
    "    print(f\"- Accuracy: {autoencoder_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {autoencoder_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {autoencoder_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {autoencoder_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {autoencoder_results['training_time']:.2f}s\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    autoencoder_test_metrics = autoencoder_model.evaluate(X_test_flat, y_test_seq)\n",
    "    print(f\"\\nAutoencoder Test Results:\")\n",
    "    print(f\"- Accuracy: {autoencoder_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {autoencoder_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {autoencoder_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {autoencoder_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {autoencoder_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "    # Extract learned features\n",
    "    encoded_features = autoencoder_model.extract_features(X_test_flat)\n",
    "    print(f\"\\nLearned feature dimensions: {encoded_features.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping Autoencoder training\")\n",
    "    autoencoder_model = None\n",
    "    autoencoder_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble of Deep Learning Models\n",
    "\n",
    "Let's create an ensemble combining our best deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from models.ensemble import EnsembleModel\n",
    "    from config import EnsembleConfig\n",
    "    \n",
    "    # Create ensemble configuration\n",
    "    ensemble_config = EnsembleConfig(\n",
    "        base_models=[\n",
    "            {'type': 'lstm', 'units': 64, 'dropout': 0.3, 'epochs': 20},\n",
    "            {'type': 'random_forest', 'n_estimators': 100, 'max_depth': 10}\n",
    "        ],\n",
    "        ensemble_method='voting',\n",
    "        voting_type='soft'\n",
    "    )\n",
    "    \n",
    "    print(\"Training Deep Learning Ensemble...\")\n",
    "    \n",
    "    # For ensemble, we'll use flattened data (can handle both types)\n",
    "    ensemble_model = EnsembleModel(ensemble_config)\n",
    "    \n",
    "    ensemble_results = trainer.train_model(\n",
    "        model=ensemble_model,\n",
    "        X_train=X_train_flat,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_flat,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"dl_ensemble_weather\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEnsemble Training Results:\")\n",
    "    print(f\"- Accuracy: {ensemble_results['accuracy']:.3f}\")\n",
    "    print(f\"- Precision: {ensemble_results['precision']:.3f}\")\n",
    "    print(f\"- Recall: {ensemble_results['recall']:.3f}\")\n",
    "    print(f\"- F1 Score: {ensemble_results['f1_score']:.3f}\")\n",
    "    print(f\"- Training Time: {ensemble_results['training_time']:.2f}s\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    ensemble_test_metrics = ensemble_model.evaluate(X_test_flat, y_test_seq)\n",
    "    print(f\"\\nEnsemble Test Results:\")\n",
    "    print(f\"- Accuracy: {ensemble_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {ensemble_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {ensemble_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {ensemble_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {ensemble_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping ensemble training\")\n",
    "    ensemble_model = None\n",
    "    ensemble_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison and Analysis\n",
    "\n",
    "Let's compare all our deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results for comparison\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    model_results = []\n",
    "    \n",
    "    # Add each model if it was trained successfully\n",
    "    if lstm_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'LSTM',\n",
    "            'Accuracy': lstm_test_metrics.accuracy,\n",
    "            'Precision': lstm_test_metrics.precision,\n",
    "            'Recall': lstm_test_metrics.recall,\n",
    "            'F1 Score': lstm_test_metrics.f1_score,\n",
    "            'AUC-ROC': lstm_test_metrics.auc_roc,\n",
    "            'Training Time (s)': lstm_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if transformer_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'Transformer',\n",
    "            'Accuracy': transformer_test_metrics.accuracy,\n",
    "            'Precision': transformer_test_metrics.precision,\n",
    "            'Recall': transformer_test_metrics.recall,\n",
    "            'F1 Score': transformer_test_metrics.f1_score,\n",
    "            'AUC-ROC': transformer_test_metrics.auc_roc,\n",
    "            'Training Time (s)': transformer_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if attention_lstm_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'Attention-LSTM',\n",
    "            'Accuracy': attention_lstm_test_metrics.accuracy,\n",
    "            'Precision': attention_lstm_test_metrics.precision,\n",
    "            'Recall': attention_lstm_test_metrics.recall,\n",
    "            'F1 Score': attention_lstm_test_metrics.f1_score,\n",
    "            'AUC-ROC': attention_lstm_test_metrics.auc_roc,\n",
    "            'Training Time (s)': attention_lstm_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if cnn_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'CNN',\n",
    "            'Accuracy': cnn_test_metrics.accuracy,\n",
    "            'Precision': cnn_test_metrics.precision,\n",
    "            'Recall': cnn_test_metrics.recall,\n",
    "            'F1 Score': cnn_test_metrics.f1_score,\n",
    "            'AUC-ROC': cnn_test_metrics.auc_roc,\n",
    "            'Training Time (s)': cnn_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if autoencoder_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'Autoencoder',\n",
    "            'Accuracy': autoencoder_test_metrics.accuracy,\n",
    "            'Precision': autoencoder_test_metrics.precision,\n",
    "            'Recall': autoencoder_test_metrics.recall,\n",
    "            'F1 Score': autoencoder_test_metrics.f1_score,\n",
    "            'AUC-ROC': autoencoder_test_metrics.auc_roc,\n",
    "            'Training Time (s)': autoencoder_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if ensemble_model is not None:\n",
    "        model_results.append({\n",
    "            'Model': 'Ensemble',\n",
    "            'Accuracy': ensemble_test_metrics.accuracy,\n",
    "            'Precision': ensemble_test_metrics.precision,\n",
    "            'Recall': ensemble_test_metrics.recall,\n",
    "            'F1 Score': ensemble_test_metrics.f1_score,\n",
    "            'AUC-ROC': ensemble_test_metrics.auc_roc,\n",
    "            'Training Time (s)': ensemble_results['training_time']\n",
    "        })\n",
    "    \n",
    "    if model_results:\n",
    "        comparison_df = pd.DataFrame(model_results)\n",
    "        comparison_df = comparison_df.round(3)\n",
    "        \n",
    "        print(\"Deep Learning Model Comparison:\")\n",
    "        print(\"=\" * 100)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Find best models\n",
    "        print(\"\\nBest Models by Metric:\")\n",
    "        print(\"=\" * 50)\n",
    "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']:\n",
    "            best_idx = comparison_df[metric].idxmax()\n",
    "            best_model = comparison_df.loc[best_idx, 'Model']\n",
    "            best_value = comparison_df.loc[best_idx, metric]\n",
    "            print(f\"{metric:>12}: {best_model} ({best_value:.3f})\")\n",
    "    else:\n",
    "        print(\"No models were successfully trained for comparison.\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available, no deep learning models to compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "if TENSORFLOW_AVAILABLE and 'comparison_df' in locals() and len(comparison_df) > 0:\n",
    "    # Performance comparison bar chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Deep Learning Model Performance Comparison', fontsize=16)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].bar(comparison_df['Model'], comparison_df['Accuracy'], color='skyblue')\n",
    "    axes[0, 0].set_title('Accuracy Comparison')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    axes[0, 1].bar(comparison_df['Model'], comparison_df['F1 Score'], color='lightgreen')\n",
    "    axes[0, 1].set_title('F1 Score Comparison')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # AUC-ROC comparison\n",
    "    axes[1, 0].bar(comparison_df['Model'], comparison_df['AUC-ROC'], color='coral')\n",
    "    axes[1, 0].set_title('AUC-ROC Comparison')\n",
    "    axes[1, 0].set_ylabel('AUC-ROC')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Training time comparison\n",
    "    axes[1, 1].bar(comparison_df['Model'], comparison_df['Training Time (s)'], color='gold')\n",
    "    axes[1, 1].set_title('Training Time Comparison')\n",
    "    axes[1, 1].set_ylabel('Training Time (seconds)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Radar chart for multi-metric comparison\n",
    "    import math\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "    angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        if i < len(colors):\n",
    "            values = [row[metric] for metric in metrics]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Deep Learning Model Performance (Radar Chart)', size=16, y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No comparison data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretation and Analysis\n",
    "\n",
    "Let's analyze what our deep learning models have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions and patterns\n",
    "if TENSORFLOW_AVAILABLE and lstm_model is not None:\n",
    "    # Get predictions from the best model\n",
    "    lstm_predictions = lstm_model.predict(X_test_seq_scaled)\n",
    "    lstm_probabilities = lstm_model.predict_proba(X_test_seq_scaled)\n",
    "    \n",
    "    # Analyze prediction confidence\n",
    "    confidence_scores = np.max(lstm_probabilities, axis=1)\n",
    "    \n",
    "    # Create prediction analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('LSTM Model Analysis', fontsize=16)\n",
    "    \n",
    "    # Prediction confidence distribution\n",
    "    axes[0, 0].hist(confidence_scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Prediction Confidence Distribution')\n",
    "    axes[0, 0].set_xlabel('Confidence Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Confidence vs accuracy\n",
    "    correct_predictions = (lstm_predictions == y_test_seq)\n",
    "    axes[0, 1].scatter(confidence_scores[correct_predictions], \n",
    "                      np.ones(sum(correct_predictions)), \n",
    "                      alpha=0.6, label='Correct', s=10)\n",
    "    axes[0, 1].scatter(confidence_scores[~correct_predictions], \n",
    "                      np.zeros(sum(~correct_predictions)), \n",
    "                      alpha=0.6, label='Incorrect', s=10, color='red')\n",
    "    axes[0, 1].set_title('Confidence vs Correctness')\n",
    "    axes[0, 1].set_xlabel('Confidence Score')\n",
    "    axes[0, 1].set_ylabel('Correct (1) / Incorrect (0)')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Prediction probability distribution\n",
    "    reg_probs = lstm_probabilities[y_test_seq == 1][:, 1]  # Regulation probability for actual regulations\n",
    "    no_reg_probs = lstm_probabilities[y_test_seq == 0][:, 1]  # Regulation probability for no regulations\n",
    "    \n",
    "    axes[1, 0].hist(no_reg_probs, bins=30, alpha=0.7, label='No Regulation (True)', color='blue')\n",
    "    axes[1, 0].hist(reg_probs, bins=30, alpha=0.7, label='Regulation (True)', color='red')\n",
    "    axes[1, 0].set_title('Predicted Probability Distribution')\n",
    "    axes[1, 0].set_xlabel('Predicted Regulation Probability')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Feature importance proxy (using gradient approximation)\n",
    "    # For demonstration, we'll show which time steps are most important\n",
    "    sample_sequences = X_test_seq_scaled[:100]  # Use first 100 test samples\n",
    "    base_pred = lstm_model.predict_proba(sample_sequences)[:, 1]\n",
    "    \n",
    "    # Perturb each time step and measure impact\n",
    "    time_importance = []\n",
    "    for t in range(sequence_length):\n",
    "        perturbed_sequences = sample_sequences.copy()\n",
    "        perturbed_sequences[:, t, :] = 0  # Zero out time step t\n",
    "        perturbed_pred = lstm_model.predict_proba(perturbed_sequences)[:, 1]\n",
    "        importance = np.mean(np.abs(base_pred - perturbed_pred))\n",
    "        time_importance.append(importance)\n",
    "    \n",
    "    axes[1, 1].plot(range(sequence_length), time_importance, marker='o')\n",
    "    axes[1, 1].set_title('Time Step Importance (Hours Before)')\n",
    "    axes[1, 1].set_xlabel('Hours Before Current Time')\n",
    "    axes[1, 1].set_ylabel('Average Impact on Prediction')\n",
    "    axes[1, 1].invert_xaxis()  # Most recent time on the right\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"Model Analysis Insights:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Average prediction confidence: {confidence_scores.mean():.3f}\")\n",
    "    print(f\"High confidence predictions (>0.9): {(confidence_scores > 0.9).mean():.1%}\")\n",
    "    print(f\"Low confidence predictions (<0.6): {(confidence_scores < 0.6).mean():.1%}\")\n",
    "    \n",
    "    most_important_time = np.argmax(time_importance)\n",
    "    print(f\"Most important time step: {sequence_length - most_important_time} hours before\")\n",
    "    print(f\"Recent hours (last 6) average importance: {np.mean(time_importance[-6:]):.4f}\")\n",
    "    print(f\"Earlier hours (first 6) average importance: {np.mean(time_importance[:6]):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"LSTM model not available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning for Deep Learning\n",
    "\n",
    "Let's demonstrate advanced hyperparameter tuning for our best deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    from training.hyperparameter_tuning import BayesianOptimizationTuner\n",
    "    \n",
    "    # Define parameter space for LSTM tuning\n",
    "    param_space = {\n",
    "        'units': [32, 128],\n",
    "        'dropout': [0.1, 0.5],\n",
    "        'recurrent_dropout': [0.1, 0.5],\n",
    "        'learning_rate': [0.0001, 0.01],\n",
    "        'batch_size': [16, 64]\n",
    "    }\n",
    "    \n",
    "    print(\"Starting Bayesian optimization for LSTM...\")\n",
    "    print(f\"Parameter space: {param_space}\")\n",
    "    \n",
    "    # Create base model for tuning\n",
    "    base_config = LSTMConfig(epochs=15, sequence_length=sequence_length)  # Fewer epochs for speed\n",
    "    tuning_model = LSTMModel(base_config)\n",
    "    \n",
    "    # Perform Bayesian optimization\n",
    "    tuner = BayesianOptimizationTuner(n_trials=10)  # Limited trials for demo\n",
    "    \n",
    "    tuning_result = tuner.tune(\n",
    "        model=tuning_model,\n",
    "        param_space=param_space,\n",
    "        X_train=X_train_seq_scaled,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_seq_scaled,\n",
    "        y_val=y_val_seq\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBayesian Optimization Results:\")\n",
    "    print(f\"Best parameters: {tuning_result.best_params}\")\n",
    "    print(f\"Best validation score: {tuning_result.best_score:.3f}\")\n",
    "    print(f\"Number of trials: {len(tuning_result.all_results)}\")\n",
    "    \n",
    "    # Train optimized model\n",
    "    optimized_config = LSTMConfig(\n",
    "        **tuning_result.best_params,\n",
    "        epochs=30,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    optimized_lstm = LSTMModel(optimized_config)\n",
    "    \n",
    "    optimized_results = trainer.train_model(\n",
    "        model=optimized_lstm,\n",
    "        X_train=X_train_seq_scaled,\n",
    "        y_train=y_train_seq,\n",
    "        X_val=X_val_seq_scaled,\n",
    "        y_val=y_val_seq,\n",
    "        model_name=\"optimized_lstm\"\n",
    "    )\n",
    "    \n",
    "    # Test optimized model\n",
    "    optimized_test_metrics = optimized_lstm.evaluate(X_test_seq_scaled, y_test_seq)\n",
    "    \n",
    "    print(f\"\\nOptimized LSTM Test Results:\")\n",
    "    print(f\"- Accuracy: {optimized_test_metrics.accuracy:.3f}\")\n",
    "    print(f\"- Precision: {optimized_test_metrics.precision:.3f}\")\n",
    "    print(f\"- Recall: {optimized_test_metrics.recall:.3f}\")\n",
    "    print(f\"- F1 Score: {optimized_test_metrics.f1_score:.3f}\")\n",
    "    print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc:.3f}\")\n",
    "    \n",
    "    # Compare with original\n",
    "    if lstm_model is not None:\n",
    "        print(f\"\\nImprovement over original LSTM:\")\n",
    "        print(f\"- Accuracy: {optimized_test_metrics.accuracy - lstm_test_metrics.accuracy:+.3f}\")\n",
    "        print(f\"- F1 Score: {optimized_test_metrics.f1_score - lstm_test_metrics.f1_score:+.3f}\")\n",
    "        print(f\"- AUC-ROC: {optimized_test_metrics.auc_roc - lstm_test_metrics.auc_roc:+.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available, skipping hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Best Practices\n",
    "\n",
    "Let's summarize our findings and provide recommendations for deep learning in weather prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print(\"Deep Learning for Weather Regulation Prediction - Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if TENSORFLOW_AVAILABLE and 'comparison_df' in locals() and len(comparison_df) > 0:\n",
    "    # Find best overall model\n",
    "    best_f1_idx = comparison_df['F1 Score'].idxmax()\n",
    "    best_model = comparison_df.loc[best_f1_idx, 'Model']\n",
    "    best_f1 = comparison_df.loc[best_f1_idx, 'F1 Score']\n",
    "    \n",
    "    print(f\"\\nð Best Overall Model: {best_model} (F1: {best_f1:.3f})\")\n",
    "    \n",
    "    # Model-specific insights\n",
    "    print(\"\\nð Model-Specific Insights:\")\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        model = row['Model']\n",
    "        if model == 'LSTM':\n",
    "            print(f\"   â¢ {model}: Good for sequential patterns, moderate training time\")\n",
    "        elif model == 'Transformer':\n",
    "            print(f\"   â¢ {model}: State-of-the-art attention, longer training time\")\n",
    "        elif model == 'Attention-LSTM':\n",
    "            print(f\"   â¢ {model}: Combines LSTM memory with attention focus\")\n",
    "        elif model == 'CNN':\n",
    "            print(f\"   â¢ {model}: Captures local patterns, fast training\")\n",
    "        elif model == 'Autoencoder':\n",
    "            print(f\"   â¢ {model}: Learns compressed representations, good for feature learning\")\n",
    "        elif model == 'Ensemble':\n",
    "            print(f\"   â¢ {model}: Combines strengths of multiple models, robust predictions\")\n",
    "    \n",
    "    print(\"\\nð¯ Key Findings:\")\n",
    "    avg_accuracy = comparison_df['Accuracy'].mean()\n",
    "    avg_f1 = comparison_df['F1 Score'].mean()\n",
    "    print(f\"   â¢ Average deep learning accuracy: {avg_accuracy:.3f}\")\n",
    "    print(f\"   â¢ Average F1 score: {avg_f1:.3f}\")\n",
    "    \n",
    "    fastest_model = comparison_df.loc[comparison_df['Training Time (s)'].idxmin(), 'Model']\n",
    "    slowest_model = comparison_df.loc[comparison_df['Training Time (s)'].idxmax(), 'Model']\n",
    "    print(f\"   â¢ Fastest training: {fastest_model}\")\n",
    "    print(f\"   â¢ Slowest training: {slowest_model}\")\n",
    "\n",
    "print(\"\\nð¬ Technical Insights:\")\n",
    "print(\"   â¢ Sequential models (LSTM, Transformer) excel at temporal patterns\")\n",
    "print(\"   â¢ Attention mechanisms help focus on critical time periods\")\n",
    "print(\"   â¢ Ensemble methods provide robust, well-calibrated predictions\")\n",
    "print(\"   â¢ CNNs can capture spatial-temporal patterns when data is reshaped\")\n",
    "print(\"   â¢ Autoencoders useful for dimensionality reduction and anomaly detection\")\n",
    "\n",
    "print(\"\\nâ¡ Performance Optimization Tips:\")\n",
    "print(\"   â¢ Use bidirectional LSTM for better context understanding\")\n",
    "print(\"   â¢ Apply dropout (0.2-0.4) to prevent overfitting\")\n",
    "print(\"   â¢ Sequence length of 12-48 hours usually optimal for weather\")\n",
    "print(\"   â¢ Batch size 32-64 provides good speed/memory trade-off\")\n",
    "print(\"   â¢ Early stopping prevents overtraining\")\n",
    "print(\"   â¢ Learning rate scheduling improves convergence\")\n",
    "\n",
    "print(\"\\nðï¸ Hyperparameter Recommendations:\")\n",
    "print(\"   â¢ LSTM units: 32-128 (balance complexity vs overfitting)\")\n",
    "print(\"   â¢ Transformer heads: 4-8 (match data complexity)\")\n",
    "print(\"   â¢ Learning rates: 0.001-0.01 (start conservative)\")\n",
    "print(\"   â¢ Use Bayesian optimization for efficient search\")\n",
    "print(\"   â¢ Monitor validation metrics to avoid overfitting\")\n",
    "\n",
    "print(\"\\nð Next Steps for Production:\")\n",
    "print(\"   â¢ Implement real-time inference pipeline\")\n",
    "print(\"   â¢ Add model monitoring and drift detection\")\n",
    "print(\"   â¢ Set up automated retraining workflows\")\n",
    "print(\"   â¢ Create ensemble of best-performing models\")\n",
    "print(\"   â¢ Implement A/B testing for model comparison\")\n",
    "print(\"   â¢ Add uncertainty quantification for predictions\")\n",
    "\n",
    "print(\"\\nð Future Enhancements:\")\n",
    "print(\"   â¢ Multi-modal fusion (weather radar + satellite imagery)\")\n",
    "print(\"   â¢ Graph neural networks for airport network effects\")\n",
    "print(\"   â¢ Federated learning across multiple airports\")\n",
    "print(\"   â¢ Explainable AI for regulatory compliance\")\n",
    "print(\"   â¢ Real-time model updates with streaming data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for exploring deep learning for weather regulation prediction!\")\n",
    "print(\"For more advanced techniques, check out the other tutorial notebooks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
